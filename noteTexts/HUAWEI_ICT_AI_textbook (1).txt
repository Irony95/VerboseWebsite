The emergence and rise of articial intelligence undoubtedly played an important role during the development of the Internet.
Over the past decade, with extensive applications in the society, articial intelligence has become more relevant to peoples daily life.
This chapter introduces the concept of articial intelligence, the related technologies, and the existing controversies over the topic.
Currently, people mainly learn about articial intelligence (AI) through news, movies, and the applications in daily life, as shown by Fig.
A rather widely accepted denition of AI, also a relatively early one, was proposed by John McCarthy at the 1956 Dartmouth Conference, which outlined that articial intelligence is about letting a machine simulate the intelligent behavior of humans as precisely as it can be.
However, this denition seemingly ignores the possibility of strong articial intelligence (which means the machine that has the ability or intelligence to solve problems by reasoning).
Before explaining what articial intelligence is, we had better clarify the concept of intelligence rst.
According to the theory of multiple intelligences, human intelligence can be categorized into seven types: Linguistic, Logical-Mathematical, Spatial Bodily- Kinesthetic, Musical, Interpersonal and Intrapersonal intelligence.
Linguistic Intelligence Linguistic intelligence refers to the ability to effectively express ones thoughts in spoken or written language, understand others words or texts, exibly master the phonology, semantics, and grammar of a language, manage verbal thinking, and convey or decode the connotation of linguistic expressions through the verbal thinking.
For the people with strong linguistic intelligence, the ideal career choices could be politician-activist, host, attorney, public speaker, editor, writer, journalist, teacher, etc.
Logical-Mathematical Intelligence Logical-mathematical intelligence designates the capability to calculate, quan- tify, reason, summarize and classify effectively, and to carry out complicated mathematical operations.
This capability is characterized by the sensitivity to abstract concepts, such as logical patterns and relationships, statements and claims, and functions.
People who are strong in logic-mathematical intelligence are more suitable to work as scientists, accountants, statisticians, engineers, computer software developers, etc.
Spatial Intelligence Spatial intelligence features the potential to accurately recognize the visual space and things around it, and to represent what they perceived visually in paintings and graphs.
People with strong spatial intelligence are very sensitive to spatial relationships such as color, line, shape, and form.
The jobs suitable for them are interior designer, architect, photographer, painter, pilot and so on.
Bodily-Kinesthetic Intelligence Bodily-kinesthetic intelligence indicates the capacity to use ones whole body to express thoughts and emotions, and to use hands and other tools to fashion products or manipulate objects.
This intelligence demonstrates a variety of particular physical skills such as balance, coordination, agility, strength, supple- ness and speed, and tactile abilities.
Potential careers for people with strong body- kinesthetic intelligence include athlete, actor, dancer, surgeon, jeweler, mechanic and so on.
Musical Intelligence Musical intelligence is the ability to discern pitch, tone, melody, rhythm, and timbre.
People having relatively high musical intelligence are particularly sensi- tive to pitch, tone, melody, rhythm or timbre, and are more competitive in performing, creating and reecting on music.
Their recommended professions include singer, composer, conductor, music critic, the piano tuner and so on.
Interpersonal Intelligence Interpersonal intelligence is the capability to understand and interact effec- tively with others.
People with strong interpersonal intelligence can better recog- nize the moods and temperaments of others, empathize with their feelings and emotions, notice the hidden information of different interpersonal relationships, and respond appropriately.
The professions suitable for them include politician, diplomat, leader, psychologist, PR ofcer, salesmen, and so on.
Intrapersonal Intelligence Intrapersonal intelligence is about self-recognition, which means the capabil- ity to understand oneself and then act accordingly based on such knowledge.
People with strong intrapersonal intelligence are able to discern their strengths and weaknesses, recognize their inner hobbies, moods, intentions, temperaments and self-esteem, and they like to think independently.
Their suitable professions include philosopher, politician, thinker, psychologist and so on.
Naturalist Intelligence Naturalist intelligence refers to the ability to observe the various forms of nature, identify and classify the objects, and discriminate the natural and articial systems.
However, AI is a new type of technological science that investigates and develops the theories, methods, technologies and application systems to simulate, improve and upgrade the human intelligence.
The AI is created to enable machines to reason like human being and to endow them with intelligence.
Today, the connotation of AI has been greatly broadened, making it an interdisciplinary subject, as shown by Fig.
Machine learning (ML) is apparently one of the major focuses of this interdisci- plinary subject.
According to the denition by Tom Mitchell, the so-called the godfather of global ML, machine learning is described as: with respect to certain type of tasks T and performance P, if the performance of a computer program at tasks in T improves with experience E as measured by P, then the computer program is deemed to learn from experience E.
It is a relatively simple and abstract denition.
However, as our perception on the concept deepened, we may nd that the conno- tation and denotation of machine learning will also change accordingly.
It is not easy to dene machine learning that precisely in only one or two sentences, not only because that it covers a wide span of elds in terms of theory and application, but also it is developing and transforming quite rapidly.
Generally speaking, the processing system and algorithms of machine learning make predictions mainly by identifying the hidden patterns from data.
It is an important sub-eld of AI, and AI is intertwined with data mining (DM) and knowl- edge discovery in database (KDD) in a broader sense.
The study of machine learning aims at enabling computers to simulate or perform human learning ability and acquire new knowledge and skills.
Deep learning (DL) derives from the study of articial neural networks (ANN). As a new subeld of machine learning, it focuses on mimicking the mechanisms of human brain in interpreting data like images, sound, and text.
The relationship between AI, machine learning, and deep learning is shown in Fig.
Among the three concepts, machine learning is an approach or a subset of AI, and deep learning is one of MLs special forms.
If we take AI as the brain, then machine learning is the process of the acquisition of cognitive abilities, and deep learning is a highly efcient self-training system that dominates this process.
Articial intelli- gence is the target and result while deep learning and machine learning are methods and tools.
AI can be divided into two types: strong articial intelligence and weak articial intelligence.
Strong articial intelligence is about the possibility to create the intelligent machines that can accomplish reasoning problem-solving tasks.
The machines of this kind are believed to have consciousness and self-awareness and be able to think independently and come up with the best solutions to the problems.
Strong AI also has its distinctive values and worldview, and is endowed with instincts, such as the needs of survival and safety, just like all the living beings.
In a certain sense, strong AI is a new civilization.
Weak articial intelligence depicts the circumstance when it is not able to make machines that can truly accomplish reasoning and problem-solving.
These machines may look smart, but they do not really have intelligence or self-awareness.
We are currently in the era of weak articial intelligence.
The introduction of weak articial intelligence reduces the burden of intellectual work by functioning in a way similar to the advanced bionics.
Whether it is AlphaGo, or the robot who writes news report and novels, they all belong to weak articial intelligence and outperform humans in only certain elds.
In the era of weak articial intelligence, it is undeniable that data and computing power are both crucial, as they can facilitate the commercialization of AI.
In the coming age of strong articial intelligence, these two factors will still be two decisive elements.
Meanwhile, the exploration on quantum computing by companies like Google and IBM also lays the foundation for the advent of the strong articial intelligence era.
Figure 1.4 presents us a brief history of AI.
The ofcial origin of modern AI can be traced back to the Turing Test proposed by Alan M.
Turing, known as the Father of Articial Intelligence, in 1950.
According to his assumption, if a computer can engage in dialogue with humans without being detected as a computer, then it is deemed as having intelligence.
In the same year he proposed this assumption, Turing boldly predicted that creating the machines with real human intelligence was possible in the future.
But up to now, none of the computers has ever passed the Turing Test.
Although AI is a concept with a history of only a few decades, the theoretical foundation and supporting technology behind it have developed for a time-honored period.
The current prosperity of AI is a result of the advancement of all related disciplines and the collective efforts of the scientists of all generations.
Precursors and Initiation Period (before 1956) The theoretical foundation for the birth of AI can date back to as early as the fourth century BC, when the famous ancient Greek philosopher and scientist Aristotle invented the concept of formal logic.
In fact, his theory of syllogism is still working as an indispensable and decisive cornerstone for the deductive reasoning today.
In the seventeenth century, the German mathematician Gottfried Leibniz advanced universal notation and some revolutionary ideas on reasoning and calculation, which laid the foundation for the establishment and development of mathematical logic.
In the nineteenth century, the British mathematician George Boole developed Boolean algebra, which is the bedrock of the operation of modern computers, and its introduction makes the invention of computer possible.
During the same period, the British inventor Charles Babbage created the Difference Engine, the rst computer capable of solving quadratic polynomial equations in the world.
Although it only had limited functions, this computer reduced the burden of human brain in calculation per se for the rst time.
The machines were endowed with computational intelligence ever since.
In 1945, John Mauchly and J.
Presper Eckert from a team at Moore School designed the Electronic Numerical Integrator and Calculator (ENIAC), the worlds rst general-purpose digital electronic computer.
As an epoch-making achievement, ENIAC still had its fatal deciencies, such as its enormous size, excessive power consumption, and reliance on manual operation to input and adjust commands.
In 1947, John von Neumann, the father of modern computers, modied and upgraded on the basis of ENIAC and created the modern electronic computer in the real sense: Mathematical Analyzer Numerical Integrator and Automatic Computer (MANIAC).
In 1946, the American physiologist Warren McCulloch established the rst model of neural network.
His research on articial intelligence at microscopic level laid an important foundation for the development of neural networks.
In 1949, Donald O.
Hebb proposed Hebbian theory, a neuropsychological learning paradigm, which states the basic principles of synaptic plasticity, namely, the efcacy of synaptic transmission will arise greatly with the repeated and persis- tent stimulation from a presynaptic neuron to a postsynaptic neuron.
This theory is fundamental to the modelling of neural networks.
In 1948, Claude E.
Shannon, the founder of information theory, introduced the concept of information entropy by borrowing the term from thermodynamics, and dened information entropy as the average amount of information after the redundancy has being removed.
The impact of this theory is quite far-reaching as it played an important role in elds such as non-deterministic inference and machine learning.
Finally, in 1956, John McCarthy ofcially introduced AI as a new discipline at the 2-month long Dartmouth Conference, which marks the birth of AI.
A number of AI research groups were formed in the United States ever since, such as the Carnegie-RAND group formed by Allen Newell and Herbert Alexander Simon, the research group the Massachusetts Institute of Technology (MIT) by Marvin Lee Minsky and John McCarthy, and Arthur Samuels IBM engineering research group, etc.
In the following two decades, AI was developing rapidly in a wide range of elds, and thanks to the great enthusiasm of researchers, the AI technologies and applications have kept expanding.
(a) Machine Learning In 1956, Arthur Samuel of IBM wrote the famous checkers-playing program, which was able to learn an implicit model by observing the posi- tions on checkerboard to instruct moves for the latter cases.
After played against the program for several rounds, Arthur Samuel concluded that the program could reach a very high level of performance during the course of 2.
The First Booming Period (19561976) learning.
With this program, Samuel confuted the notion that computers cannot go beyond the written codes and learn patterns like human beings.
Since then, he coined and dened a new termmachine learning.
(b) Pattern Recognition In 1957, C.K.
Chow proposed to adopt statistical decision theory to tackle pattern recognition, which stimulated the rapid development of pattern rec- ognition research since the late 1950s.
In the same year, Frank Rosenblatt proposed a simplied mathematical model that imitated the recognition pattern of human brainthe perceptron, the rst machine that could possibly train the recognition system by the sample of each given category, so that the system was able to correctly classify patterns of other unknown categories after learning.
(c) Pattern Matching In 1966, ELIZA, the rst conversation program in the world was invented, which was written by the MIT Articial Intelligence Laboratory.
The pro- gram was able to perform pattern matching on the basis of the set rules and users questions, so as to give appropriate replies by choosing from the pre-written answer archives.
This is also the rst program try to have passed the Turing Test.
ELIZA once masqueraded as a psychotherapist to talk to patients, and many of them failed to recognize it as a robot when it was rstly applied.
Conversation is pattern matching, thus this unveiled the computer natural language conversation technology.
In addition, during the rst development period of AI, John McCarthy developed the LISP, which became the dominant programming language for AI IN the following decades.
Marvin Minsky launched a more in-depth study of neural networks and discovered the disadvantages of simple neural net- works.
In order to overcome these limitations, the scientists started to intro- duce multilayer neural networks and Back Propagation (BP) algorithms.
Meanwhile, the expert system (ES) also emerged.
During this period the rst industrial robot was applied on the production line of General Motors, and the world also witnessed the birth of the rst mobile robot which was capable of actioning autonomously.
The advancement of relevant disciplines also contributed to the great strides of AI.
The emergence of bionics in the 1950s ignited the research enthusiasm of scientists, which led to the invention of simulated annealing algorithm.
It is a type of heuristic algorithm, and is the foundation for the searching algorithms, such as the ant colony optimization (ACO) algorithm which is quite popular in recent years.
The First AI Winter (19761982) However, the AI manic did not last too long, as the over-optimistic projections failed to be fullled as promised, and thus incurred the doubt and suspicion on AI technology globally.
The perceptron, once a sensation in the academic world, had a hard time in 1969 when Marvin Minsky and the rest scientists advanced the famous logical operation exclusive OR (XOR), demonstrating the limitation of the perceptron in terms of the linear inseparable data similar to the XOR problem.
For the academic world, the XOR problem became an almost undefeatable challenge.
In 1973, AI was under strict questioning by the scientic community.
Many scientists believed that those seemingly ambitious goals of AI were just some unfullled illusions, and the relevant research had been proved complete failures.
Due to the increasing suspicion and doubts, AI suffered from severe criticism, and its actual value was also under question.
As a consequence, the governments and research institutions all over the world withdrew or reduced funding on AI, and the industry encountered its rst winter of development in the 1970s.
The setback in 1970s was no coincidence.
Due to the limitation of computing power at that time, although many problems were solved theoretically, they cannot be put into practice at all.
Meanwhile, there were many other obstacles, such as it was difcult for the expert system to acquire knowledge, leaving lots of projects ended in failure.
The study on machine vision took off in the 1960s.
And the edge detection and contour composition methods proposed by the American scientist L.R.
Roberts are not only time-tested, but also still widely used today.
However, having a theoretical basis does not mean actual yield.
In the last 1970s, there were scientists concluded that to let a computer to imitate human retinal vision, it would need to execute at least one billion instructions.
However, the calculation speed of the worlds fastest supercomputer Cray-1 in 1976 (which costed millions of US dollars to make) could only register no more than 100 mil- lion times per second, and the speed of an ordinary computer could meet even no more than one million times per second.
The hardware limited the development of AI.
In addition, another major basis for the progress of AI is the data base.
At that time, computers and the Internet were not as popular as today, so there were nowhere for the developers to capture massive data.
During this period, articial intelligence developed slowly.
Although the concept of BP had been proposed by Juhani Linnainmaa in the automatic differential ip mode in the 1970s, it was not until 1981 that it was applied to the multilayer perceptron by Paul J.
Werbos.
The invention of multilayer perceptron and BP algorithm contributed to the second leap-frogging of neural networks.
In 1986, David E.
Rumelhart and other scholars developed an effective BP algorithm to successfully train multilayer perceptron, which exerted a pro- found impact.
The Second Booming Period (19821987) In 1980, XCON, a complete expert system developed by the Carnegie Mello University (CMU) was ofcially put into use.
The system contained more than 2500 set rules, and processed more than 80,000 orders featuring an accuracy of over 95% in the following years.
This is considered a milestone that heralds a new era, when the expert system begun to showcase its potential in specic elds, which lifted AI technology to a completely new level of booming development.
An expert system normally attends to one specic professional eld.
By mimicking the thinking of human experts, it attempts to answer questions or provide knowledge to help with the decision-making by practitioners.
Focusing on only a narrow domain, the expert system avoids the challenges related to articial general intelligence (AGI) and is able to make full use of the knowledge and experience of existing experts to solve problems of the specic domains.
The big commercial success of XCON encouraged 60% of Fortune 500 com- panies to embark on the development and deployment of expert systems in their respective elds in the 1980s.
According to the statistics, from 1980 to 1985, more than 1 billion US dollars was invested in AI, with a majority went to the internal AI departments of those enterprises, and the market witnessed a surge in AI software and hardware companies.
In 1986, the Bundeswehr University in Munich equipped a Mercedes-Benz van with a computer and several sensors, which enabled an automatic control of the steering wheel, accelerator and brake.
The installation was called VaMoRs, which proved to be the rst self-driving car in the real sense in the world.
LISP was the mainstream programming language used for AI development at that time.
In order to enhance the operating efciency of LISP programs, many agencies turned to develop computer chips and storage devices designed specif- ically to executive LISP programs.
Although LISP machines had made some progress, personal computers (PCs) were also on the rise.
IBM and Apple quickly expanded the market presence in the entire computer marketplace.
With a steady increase of CPU frequency and speed, the PCs were becoming even more powerful than the costly LISP machines.
The Second AI Winter (19871997) In 1987, along with the crash of sales market of LISP machine hardware, the AI industry once again fell into another winter.
The second AI trough period lasted for years as the hardware market collapsed and governments and institu- tions all over the world stopped investing in AI research.
But during this period, the researchers still made some important achievements.
In 1988, the American scientist Judea Pearl championed the probabilistic approach to AI inference, which made a crucial contribution to the future development of AI technology.
In the almost 20 years after the advent of the second AI winter, the AI technology became gradually and deeply integrated with computer and software technologies, while the research on articial intelligence algorithm theory had a slow progress.
The research results of many researchers were only something based on the old theories, and the computer hardware that was more powerful and faster.
Recovery Period (19972010) In 1995, Richard S.
Wallace was inspired by ELIZA and developed a new chatbot program named A.L.I.C.E. (the Articial Linguistic Internet Computer Entity). The robot was able to optimize the contents and enrich its datasets automatically through the Internet.
In 1996, the IBM supercomputer Deep Blue played a chess game against the world chess champion Gary Kasparov and was defeated.
Gary Kasparov believed that it was impossible for computers to defeat human in chess games ever.
After the match, IBM upgraded Deep Blue.
The new Deep Blue was enhanced with 480 specialized CPUs and a doubled calculation speed up to 200 million times per second, enabling it to predict the next 8 or more moves on the chessboard.
In the later rematch, the computer defeated Gary Kasparov successfully.
However, this landmark event actually only marks a victory of computer over human in a game with clear rules by relying on its calculation speed and enumeration.
This is not real AI.
In 2006, as Geoffrey Hinton published a paper in Science Magazine, AI industry entered the era of deep learning.
Rapid Growth Period (2010present) In 2011, the Watson system, also a program from IBM, participated the quiz show Jeopardy, competing with human players.
The Watson system defeated two human champions with its outstanding natural language processing capabilities and powerful knowledge database.
This time, computers can already comprehend human language, which is a big advancement in AI.
In the twenty-rst century, with the widespread application of PCs and the burst of mobile Internet and cloud computing technology, the institutions are able to capture and accumulate an unimaginably huge mass of data, providing suf- cient material and impetus for the ongoing development of AI.
Deep learning became a mainstream of AI technology, exemplied by the famous Google Brain project, which enhanced the recognition rate of the ImageNet dataset to 84% by a large margin.
In 2011, the concept semantic network was proposed.
The concept steams from the World Wide Web.
It is essentially a large-scale distributed database that centers on Web data and connects Web data in the method of machine under- standing and processing.
The emergence of the semantic network greatly pro- moted the progress of technology of knowledge representation.
A year later, Google rst announced the concept of knowledge graph and launched a knowledge-graph-based searching service.
In 2016 and 2017, Google launched two Go competitions between human and mechanical players that shocked the world.
Its AI program AlphaGo defeated two Go world champions, rst Lee Sedol of South Korea and then Ke Jie of China.
Today, AI can be found in almost all aspects of peoples life.
For instance, the voice assistant, such as the most typical Siri of Apple, is based on the natural language processing (NLP) technology.
With the support of NLP, computers can process human language and match it with the commands and responses in line with human expectation more and more naturally.
When users are browsing e-commerce websites, they could possibly receive product recommendation feeds generated by a recommendation algorithm.
The recommendation algorithm can predict the products that the users might want to buy by reviewing and analyzing the historical data of the users recent purchases and preferences.
Currently, symbolism, connectionism, and behaviorism constitute the three main schools of AI.
The following passages will introduce them in detail.
Symbolism The basic theory of symbolism believes that, the cognitive process of human being consists of the inference and processing of symbols.
Human is an example of physical symbol system, and so does the computer.
Therefore, computers should be able to simulate human intelligent activities.
And knowledge represen- tation, knowledge reasoning, and knowledge application are three crucial to articial intelligence.
Symbolism argues that knowledge and concepts can be represented by symbols, thus cognition is a process of processing the symbols, and reasoning is a process of solving problems with heuristic knowledge.
The core of symbolism lies in reasoning, namely the symbolic reasoning and machine reasoning.
Connectionism The foundation of connectionism is that the nature of human logical thinking is neurons, rather than a process of symbol processing.
Connectionism believes that the human brain is different from computers, and put forward a connectionist model imitating brain work to replace the computer working model operated by symbols.
Connectionism is believed to stem from bionics, especially in the study of human brain models.
In connectionism, a concept is represented by a set of numbers, vectors, matrices, or tensors, namely, by the specic activation mode of the entire network.
Each node (neuron) in the network has no specic meaning, but every node all participates in the expression of overall concept.
For example, in symbolism, the concept of a cat can be represented by a cat node or a group of nodes that feature the attributes of a cat (e.g., the one with two eyes, four legs or uffy). However, connectionism believes that each node does not have a specic meaning, so it is impossible to search for a cat node or eye neuron.
The core connectionism lies in neuron networks and deep learning.
Behaviorism The fundamental theory of behaviorism believes that intelligence depends on perception and behavior.
Behaviorism introduces a perception-action model for intelligent activities.
Behaviorism believes that intelligence has nothing to do with knowledge, representation, or reasoning.
AI can evolve gradually like human intelligence, and intelligent activities can only be manifested through humans ongoing interactions with the surrounding environment in the real world.
Behaviorism emphasizes application and practices and constantly learning from the environment to modify the activities.
The core behaviorism lies in behavior control, adaptation and evolutionary computing.
AI technology is multi-layered, running through technical levels such as applica- tions, algorithms, chips, devices, and processes, as shown in Fig.
AI technology has achieved the following developments at all technical levels.
Application Level Video and image: face recognition, target detection, image generation, image retouching, search image by image, video analysis, video review, and aug- mented reality (AR).
Speech and voice: speech recognition, speech synthesis, voice wake-up, voice- print recognition, and music generation.
Text: text analysis, machine translation, human-machine dialogue, reading com- prehension and recommender system.
Control: autonomous driving, drones, robots, industrial automation.
Algorithm Level Machine learning algorithms: neural network, support vector machine (SVM), K-nearest neighbor algorithm (KNN), Bayesian algorithm, decision tree, hidden Markov model (HMM), ensemble learning, etc.
Common optimization algorithms for machine learning: gradient descent, Newtons method, quasi-Newton method, conjugate gradient, spiking timing dependent plasticity (STDP), etc.
Deep learning is one of the most essential technologies for machine learning.
The deep neural network (DNN) is a hotspot of research in this eld in recent years, consisting of multilayer perceptron (MLP) and convolutional neural net- work (CNN), recurrent neural network (RNN), spiking neural network (SNN) and other types.
While the relatively popular CNNs include AlexNet, ResNet amd VGGNet, and the popular RNNs include long short-term memory (LSTM) networks and Neural Turing Machine (NTM). For instance, Googles BERT (Bidirectional Encoder Representation from Transformers) is a natural language processing pre-training technology developed on the basis of neural networks.
learning, transfer one-shot learning and adversarial machine learning are also the important tech- nologies to realize machine learning, and the solutions to some of the difculties faced by deep learning.
In addition to deep learning, learning, reinforcement 3.
Chip Level Algorithm optimization chips: performance optimization, low power consump- tion optimization, high-speed optimization, and exibility optimization-such as deep learning accelerator and face recognition chip.
Neuromorphic chips: bionic brain, biological brain-inspired intelligence, imita- tion of brain mechanism.
Programmable chips: taking exibility, programmability, algorithm compatibil- ity, and general software compatibility into consideration, such as digital signal processing (DSP) chips, graphics processing units (GPUs), eld pro- grammable gates array (FPGA).
Structure of system on chip: multi-core, many-core, single instruction, multiple data (SIMD), array structure of operation, memory architecture, on-chip network structure, multi-chip interconnection structure, memory interface, communication structure, multi-level cache.
Development toolchain: connection between deep learning frameworks (TensorFlow, Caffe, MindSpore), compiler, simulator, optimizer (quantization and clipping), atomic operation (network) library.
Device Level High-bandwidth off-chip memory: high-bandwidth memory (HBM), dynamic random-access memory (DRAM), high-speed graphics double data rate mem- ory (GDDR), low power double data rate (LPDDR SDRAN), spin-transfer torque magnetic random-access memory (STT-MRAM).
High-speed interconnection devices: serializer/deserializer (SerDes), optical interconnection communication.
Bionic devices (articial synapses, articial neurons): memristor.
New type computing devices: analog computing, in-memory computing (IMC).
Process Level On-chip memory (synaptic array): distributed static random-access memory (SRAM), Resistive random-access memory (ReRAM), and phase change random-access memory (PCRAM).
CMOS process: technology node (16 nm, 7 nm).
CMOS multi-layer integration: 2.5D IC/SiP technology, 3D-Stack technology and Monolithic 3D.
New type processes: 3D NAND, Flash Tunneling FETs, FeFET, FinFET.
The introduction of deep learning frameworks has made deep learning easier to build.
With the deep learning framework, we do not need to rstly code complex neural networks with backpropagation algorithms, but can just congure the model hyperparameters according to our demands, and the model parameters can be learned automatically from training.
We can also add a custom layer for the existing model, or choose the classier and optimization algorithm we need at the top.
We can consider a deep learning framework as a set of building blocks.
Each block, or component of the set is a model or algorithm, and we can assemble the components into an architecture that meets the demands.
The current mainstream deep learning frameworks include: TensorFlow, Caffe, PyTorch and so on.
In the four key elements of AI technology (data, algorithms, computing power, and scenarios), computing power is the one most reliant on AI processor.
Also known as AI accelerator, an AI processor is a specialized functional module to tackle the large- scale computing tasks in AI applications.
Types of AI Processors AI processors can be classied into different categories from different per- spectives, and here we will take the perspectives of technical architecture and functions.
In terms of the technical architecture, AI processors can be roughly classied into four types.
(a) CPU Central processing unit (CPU) is a large-scale integration circuit, which is the core of computing and control of a computer.
The main function of CPU is to interpret program instructions and process data in software that it receives from the computer.
(b) GPU Graphics processing unit (GPU), also known as display core (DC), visual processing unit (VPU) and display chip, is a specialized microprocessor dealing with image processing in personal computers, workstations, game consoles and some mobile devices (such as tablets and smartphones).
(c) ASIC Application specic integrated circuit (ASIC) is designed for the inte- grated circuit product customized for a particular use.
(d) FPGA Field programmable gate array (FPGA) is designed to build recongurable semi-custom chips, which means the hardware structure can be adjusted and re-congured exibly real-time as required.
In terms of the functions, AI processors can be classied into two types: training processors and inference processors.
(a) In order to train a complex deep neural network model, the AI training usually entails the input of a large amount of data and learning methods such as reinforcement learning.
Training is a compute-intensive process.
The large- scale training data and the complex deep neural network structure that the training involves put up a huge challenge to the speed, accuracy, and scal- ability of the processor.
The popular training processors include NVIDIA GPU, Googles tensor processing unit (TPU), and Huaweis neural-network processing unit (NPU).
(b) Inference here means inferring various conclusions with new data obtained on the basis of the trained model.
For instance, the video monitor can distinguish whether a captured face is the specic target by making use of the backend deep neural network model.
Although inference entails much less computation than training, it still involves lots of matrix operations.
GPU, FPGA and NPU are commonly used in inference processors.
Current Status of AI Processor (a) CPU The improvement of CPU performance in the early days mainly relied on the progress made by the underlying hardware technology in line with Moores Law.
In recent years, as Moores Law seems gradually losing its effectiveness, the development of integrated circuits is slowing down, and the hardware technology has faced physical bottlenecks.
The limitation of heat dissipation and power consumption restricted the CPU performance and serial program efciency under the traditional architecture from making much progress.
The status quo of the industry prompted researchers to keep on looking for CPU architectures and the relevant software frameworks that can better adapted to the post-Moore Era.
As a result, the multi-core processor came into being, which allows higher CPU performance with more cores.
Multi- core processors can better meet the demands of software on hardware.
For example, Intel Core i7 processors adopt instruction-level parallel processors with multiple independent kernels on the x86 instruction set, which improves the performance considerably, but also leads to higher power consumption and cost.
Since the number of cores cannot be increased indenitely, and most traditional programs are written in serial programming, this approach has limited improvements in CPU performance and program efciency.
In addition, AI performance can be improved by adding instruction set.
For example, adding instruction sets like AVX512 to the x86 complex instruction set computer (CISC), architecture, adding the fused-multiply-add (FMA) instruction set to the arithmetic logic unit (ALU) module, and adding instruction set (RISC) architecture.
to the ARM reduced instruction set computer The CPU performance can also be improved by increasing the frequency, but there is a limit, and the high frequency will cause excessive power consumption and high temperature.
(b) GPU GPU is very competitive in matrix computing and parallel computing and serves as the engine of heterogeneous computing.
It was rst introduced into the eld of AI as an accelerator to facilitate deep learning and now has formed an established ecology.
With regard to the GPUs in the eld of deep learning, NVIDIA made efforts mainly in the following three aspects: Enrich ecology: NVINIA launches the NVIDIA CUDA deep neural net- work horary (CUDNN), the GPU-accelerated library customized for deep learning, which optimizes the underlying architecture of GPU and ensures an easier application of GPU in deep learning.
Improve customization: embracing multiple data types (no longer insisting on oat32, and adopting int8, etc.).
Add module specialized for deep learning (e.g., NVIDIA V100 Tensor Core GPU adopts the improved Volta architecture introducing and equipped with tensor cores).
The main challenges of current GPUs are high cost, low energy con- sumption ratio, and high input and output latency.
(c) TPU Since 2016, Google has been committed to applying the concept of application-specic integrated circuits (ASIC) to the study of neural net- works.
In 2016, it launched the AI custom-developed processor TPU which supports the open-source deep learning framework TensorFlow.
By combin- ing large-scale systolic arrays and high-capacity on-chip memory, TPU manages to efciently accelerate the convolutional operations that are most common in deep neural networks: systolic arrays can optimize matrix multi- plication and convolutional operations, so as to increase computing power and reduce energy consumption.
(d) FPGA FPGA uses a programmable hardware description language (HDL), which is exible, recongurable, and can be deeply customized.
It can load DNN model on the chips to perform low-latency operation by incorporating mul- tiple FPGAs, contributing to a computing performance higher than GPU.
But as it has to take account the constant erasing process, the performance of FPGA cannot reach the optimal.
As FPGA is recongurable, its risk of supply and RD is relatively low.
The cost of hardware is decided by the amount of hardware purchased, so it is easy to control the cost.
However, the design of FPGA and the tape-out process are decoupled, so the development cycle is long, which usually takes half a year, and has high standards.
Comparison Between the Design of GPU and CPU The GPU is generally designed to tackle large-scale data that are highly unied in type and independent from each other, and deal with a pure computing environment without interruption.
The CPU is designed more general-purpose, so as to process different types of data, and perform logical decisions at the same time, and it also needs to introduce a large number of branch-jump instructions and interrupt processing.
The comparison between CPU and GPU architecture is shown in Fig.
The GPU has numerous massively parallel computing architectures composed by thousands of much smaller cores (designed for simultaneous processing of multiple tasks). The CPU consists of several cores optimized for serial processing.
(a) The GPU works with many ALUs and little cache memory.
Unlike the CPU, cache of the GPU serves for threads merely and plays the role of data forwarding.
When multiple threads need to access the same data, the cache will coalesce these accesses, then access the DRAM, and forward the data to each thread after obtaining them, which will cause latency.
However, as the large number of ALUs ensure the threads run in parallel, the latency is eased.
In addition, the control units of GPUs can coalesce access.
(b) The CPU has powerful ALUs, which can complete computation in a very short clock cycle.
The CPU has a large number of caches to reduce latency, and the complicated control units that can perform branch prediction and data forwarding: when a program has multiple branches, the control units will reduce latency through branch prediction for the instructions that depend on the results of previous instructions, the control units must determine the positions of these instructions in the pipeline and forward the result of the previous instruction as quickly as they can.
GPUs are good at dealing with operations that are intensive and easy to be run in parallel, while CPUs excel at logic control and serial operations.
The difference in architecture between GPU and CPU is because that they have different emphasis.
The GPU has an outstanding advantage in processing the parallel computing of large-scale intensive data, while CPU more stresses the logic control while executing the instructions.
In order to optimize a program, it often needs to coordinate both CPU and GPU at the same time to give a full play to their capabilities.
Huawei Ascend AI Processor NPU refers to the processor carrying out the optimization design specialized for neural network computing, whose performance of neural network tasks processing is much higher than that of CPU and GPU.
The NPU mimics human neurons and synapses on the circuitry, and directly processes large scale neurons and synapses through deep learning processor instruction set.
In NPUs, the processing of a group of neurons will take only one instruction.
Currently, the typical examples of NPU include Huawei Ascend AI processor, the Cambrian chip and IBMs TrueNorth chip.
There are two kinds of Huawei Ascend AI processor: Ascend 310 and Ascend 910.
Ascend 910 is mainly applied to training scenarios, mostly deployed in the data center.
While Ascend 310 is mainly designed for reasoning scenarios, whose deployment covers the device, edge and cloud full scenarios.
Ascend 910 is currently the AI processor with the strongest computing power and fastest training speed in the world, its computing power is twice that of the interna- tional top AI processor, equivalent to 50 latest and strongest CPUs today.
The relevant parameters of Ascend 310 and Ascend 910 are shown in Table 1.1.
Over the past half a century, the world has witnessed three waves of AI.
And these three waves are exemplied and unveiled by human-computer matches.
The rst was in 1962, when the checkers-playing program developed by Arthur Samuel from IBM defeated the worlds best checkers player in the United States.
The second time was in 1997, when IBMs supercomputer Deep Blue defeated the human chess world champion Garry Kasparov by 3.5:2.5.
And the third wave of AI came in 2016 when the Go AI AlphaGo developed by DeepMind, a subsidiary of Google, defeated the Go world champion and nine-dan player from the South Korean, Lee Sedol.
In the future, AI will be embedded in every walk of life, from automobiles, nance, consumer goods to retail, healthcare, education, manufacturing, communi- cations, energy, tourism, culture and entertainment, transportation, logistics, real estate and environmental protection, etc.
For example, in the automobile industry, the intelligent driving technologies such as assisted driving, assisted decision-making, and fully automated driving are all realized with the help of AI.
As a huge market, the intelligent driving can also support technical research in AI in return, thus form a virtuous circle, and become a solid foundation to the AI development.
As for the nancial industry, with the huge amount of data accumulated, AI can help with intelligent asset management, robo-advisor, and making more sensible nancial decisions.
AI can also play a part in combat nancial fraud and be adopted in anti-fraud and anti-money laundering campaigns, as the related AI program can infer the reliability of a transaction by analyzing data and materials of all kinds, to predict where the funds will ow to, and identify the cycles of the nancial market.
AI can also be widely used in the healthcare industry.
For instance, it can assist doctors to diagnose and treat diseases by identifying problems reected by the X-ray images, after being trained to interpret images at geometric level.
AI can distinguish cancer cells from normal cells after training on classication tasks.
The related research data show that by 2025, the size of AI market will exceed 3 trillion US dollars, as shown in Fig.
As can be inferred from Fig.
1.7, the AI market has a huge potential.
It is known that AI has three pillars, namely, data, algorithms and computing power.
But to apply AI in real life, these three pillars are far from enough, because we also need to take scenarios into consideration.
Data, algorithms, and computing power can prompt the evolvement of AI technically, but without application scenarios, the technological development is merely about data.
We need to integrate AI with cloud computing, big data and the Internet of Things (IoT) so as to make the application of AI in real life possible, which is the foundation of the platform architecture for AI application, as shown in Fig.
The infrastructure includes smart sensors and smart chips, which reinforces the computing power for AI industry, and guarantees its development.
AI technological service is mainly about building up an AI technological platform and providing solutions and services to external users.
The manufacturers of these AI technologies are critical in the AI industry chain, as they provide key technological platforms, solutions and services to all kinds of AI applications thanks to strong infrastructure and massive data that they acquire.
With the acceleration of the campaign of building a competitive China by developing manufacturing, Internet, and digital industry, the demand for AI in manufacturing, houseware, nance, education, transportation, security, medical care, logistics and other elds will be further released, and the AI products will have more and more diversied forms and categories.
Only when the infrastructure, the four major elements of AI and AI technical services converge, can the architecture fully buttress the upper-layer application of the AI industrial ecosystem.
Although the AI technology can be applied in a wide range of elds, its devel- opment and application are facing challenges as well: the unbalance between the limited AI development and the huge market demands.
Currently, the development and application of AI needs to deal with the following three problems.
High occupational standards: To get engaged in AI industry, it is a prerequisite for a person to have considerable knowledge in machine learning, deep learning, statistics, linear algebra and calculus.
Low efciency.
Training a model will take a long working cycle, which consists of data collection, data cleaning, model training and tuning, and optimization of visualization experience.
Fragmented capabilities and experiences: to apply a same AI model in other scenarios requires will need to repeat data collection, data cleaning, model training and tuning, and experience optimization, and the capabilities of the AI model cannot be directly passed to the next scenario.
Difcult capacity upgrading and improvement: the model upgrading and effec- tive data capturing are difcult tasks.
Currently, the smartphone-centered on-device AI has become a consensus of the industry.
More and more smartphones will boast AI capabilities.
As several con- sulting agencies in the UK and the USA estimated, about 80% of the worlds smartphones will have AI capabilities by 2022 or 2023.
To meet the market outlook and tackle the challenges of AI, HUAWEI launched its open AI capability platform for smart devices, namely, HUAWEI HiAI.
With a mission of providing developers with convenience while connecting unlimited possibilities through AI, HUAWEI HiAI enables developers to provide users with a better experience of smart applica- tion by swiftly making use of Huaweis powerful AI processing capabilities.
An Overview of Huawei CLOUD Enterprise Intelligence Application Platform Huawei CLOUD Enterprise Intelligence (EI) application platform is an enabler of enterprise intelligence that aims at providing open, credible and smart platforms based on AI and big data technologies and in the form of cloud service (including public cloud and customized cloud, etc.), By combining the industrial scenarios, the enterprise application systems created with Huawei CLOUD are visualized, audible and can express themselves, featuring the capabilities to analyze and interpret images, videos, languages and texts and easier access to AI and big data services.
Huawei CLOUD can help the enterprises to speed up business devel- opment and benet the society.
Features of Huawei CLOUD EI Huawei CLOUD EI has four remarkable features.
(a) Industrial wisdom: Huawei CLOUD has a deep understanding of the indus- try, the industrial know-how, and the major industrial deciencies.
It searches solutions to the problems in the AI technologies and navigate the implemen- tation of AI.
(b) Industrial data: It enables the companies to utilize their own data to create massive value through data processing and data mining.
(c) Algorithms: It provides enterprises with extensive algorithm libraries and model libraries, and solutions to corporate problems through general AI services and one-stop development platform.
(d) Computing power: Based on Huaweis 30 years of experiences in ICT, the full-stack AI development platform can provide enterprises with the strongest and most economical AI computing power for fusion and changes.
The History of Huawei CLOUD EI The evolvement of Huawei CLOUD EI is shown in Fig.
The evolvement of Huawei CLOUD EI is as follows.
(a) In 2002, Huawei began to develop products of data governance and analysis targeting the traditional Business Intelligence (BI) operations in the eld of telecommunication.
(b) In 2007, Huawei initiated the Hadoop technology research project, mapping out big data-related strategies, and building a pool of relevant professionals and technology patents.
(c) In 2011, Huawei tried to apply the big data technology in telecom big data solutions to deal with the network diagnosis and analysis, network planning, and network tuning.
(d) In 2013, some large companies such as China Merchants Bank and Industrial and Commercial Bank exchanged views with Huawei regarding their big data-related demands and kicked off technical cooperation.
In September of the same year, Huawei launched its enterprise-grade big data analysis plat- form FusionInsight at the Huawei Cloud Congress (HCC), which has been adopted by a wide range of industries.
(e) In 2012, Huawei ofcially stepped into AI industry and productized the research outcomes successively since 2014.
By the end of 2015, the products developed for nance, supply chain, acceptance of engineering work, and e-commerce began to put into use internally, with the following achievements accomplished.
Optical character recognition (OCR) for customs declaration documents recognition: The import efciency was enhanced by 10 times.
Delivery route planning: Additional fees were reduced by 30%.
Intelligent auditing: The efciency was increased by 6 times.
Intelligent recommendation for e-commerce users: Application conversion rate was increased by 71%.
(f) In 2017, Huawei ofcially engaged in EI services in the form of cloud service and cooperated with more partners to provide more diversied AI services to the external users.
(g) In 2019, Huawei CLOUD EI started to emphasize the inclusive AI, in the hope of making AI affordable and accessible, and safe to use.
Based on the self-developed chip Ascend, it provided 59 cloud services (21 platform services, 22 vision services, 12 language services and 4 decision-making services) and developed 159 functions (52 platform functions, 99 application programming interface API functions and 8 pre-integrated solutions).
Thousands of developers of Huawei were engaged in the technology RD projects mentioned above (including the research and development of product technology, and the cutting-edge technologies such as analysis algorithms, language processing), while machine learning algorithms, and natural Huawei also actively shared the outcomes with the Huawei AI research community in return.
As shown by Fig.
1.10, the AI technologies mainly include three types of applicational technologies of computer vision, speech processing and natural lan- guage processing.
Computer Vision Computer vision is a science that explores how to make computers see things, and the most established technology among the three genres of AI application technologies.
The subjects that computer vision mainly deals with include image classication, object detection, image segmentation, visual track- ing, text recognition and facial recognition.
Currently, computer vision is gener- image ally used in electronic attendance tracking, recognition and image search, as shown in Figs.
1.11, 1.12, 1.13 and 1.14.
In the future, computer vision will be upgraded to a more advanced level that it is capable to interpret, analyze images and make decisions autonomously, thus truly endow machines with the capability to see, and play a greater role in scenarios such as unmanned vehicles and smart homes.
identity verication, 2.
Speech Processing Speech processing is the study of the statistical characteristics of speech signals and voice production.
The processing technologies such as speech recog- nition, speech synthesis and speech wake-up can collectively be addressed as speech processing.
The sub-domains of speech processing research majorly include speech recognition, speech synthesis, voice wake-up, voiceprint recog- nition and sound event detection.
And the most mature sub-domain is the speech recognition, which can achieve an accuracy rate of 96% premised on a quiet indoor environment and near-eld recognition.
At present, the speech recognition technology is mainly used in intelligent question answering and intelligent navigation, as shown in Figs.
1.15 and 1.16.
Natural Language Processing (NLP) Natural language processing is a technology aiming at interpreting and utiliz- ing natural language through computer technologies.
The subjects of NLP include machine translation, text mining and sentiment analysis.
Faced with a number of technical challenges, NLP is not yet a very mature technology currently.
Due to the high complexity of semantics, it is impossible for AI to rival human in understanding semantics only by the deep learning based on big data and parallel computing.
In the future, AI is excepted to develop to a stage that it can automatically extract features and understand deep semantics from the current status that can only understand shallow semantics to, and to upgrade from single intelligence (machine learning) to hybrid intelligence (machine learning, deep learning and reinforcement learning). The NLP technology is now widely applied in the elds such as public opinion analysis, comment analysis and machine translation, as shown in Figs.
1.17, 1.18 and 1.19.
The applications of AI are as follows.
Smart City A smart city is to use ICT technologies to sense, analyze, and integrate the key information of the core urban operation system, so as to intelligently respond to the citys demands in peoples livelihood, environmental protection, public safety, urban services, and industrial and commercial activities.
The nature of smart city is to realize an intelligent management and operation of the city through advanced information technology, thereby improving the living stan- dards of the citizens, and promoting a harmonious and sustainable development for the city.
In a smart city, AI is mainly exemplied as smart environment, smart economy, smart life, smart information, smart supply chain and smart govern- ment.
To put it more specically, AI technologies are adopted by trafc moni- toring logistics, and facial recognition for security and protection.
Figure 1.20 shows the structure of a smart city.
Smart healthcare We can enable AI to learn professional medical knowledge, to memorize loads of health records, and to analyze medical images with computer vision, so as to provide doctors with reliable and efcient assistance, as shown in Fig.
For example, for the medical imaging widely used today, AI can build models based on historical data to analyze the medical images and quickly detect the lesions, thus improving the efciency of consultation.
Smart Retail AI will also revolutionize the retail industry.
A typical case is the unmanned supermarket.
Amazons unmanned supermarket Amazon Go adopts sensors, cameras, computer vision, and deep learning algorithms and cancels the tradi- tional check-out, so that customers can just walk in the store, grab the products they need and go.
One of the major challenges faced by unmanned supermarket is how to charge the customers correctly.
Up to now Amazon Go is the only successful case, but it is also achieved with many preconditions.
For instance, Amazon Go is only open to Amazons Prime members.
Other companies will need to build their own membership system rst if they want to follow Amazons model.
Smart security It is easier for AI to be implemented in the eld of security, and the develop- ment of AI in this eld is relatively mature.
The excessive security-related image and video data have provided a good foundation for the training of AI algorithms and models.
In the security domain, the application of AI technology can be classied as for civilian-use and for police-use.
For civilian use: facial recognition, early warning of potential dangers, home defense, and so on.
For police-use: identication of suspicious targets, vehicle analysis, tracking suspects, searching and comparing criminal suspects, entrance guard of the key supervised areas, etc.
Smart home Smart home refers to a IoT technology-based home ecosystem of hardware, software and cloud platforms, which provides users with customized life services and a more convenient, comfortable and safe living environment a home.
The smart housewares are designed to be controlled by the voice processing technology, such as adjusting the temperature of the air conditioner, opening the curtains and controlling the lighting system.
Home security is relied on the computer vision technology, such as unlocking through facial or ngerprint recognition, real-time smart camera monitoring, and detection of illegal intrusion to the residence.
With the help of machine learning and deep learning, the smart home can build user portraits and make recommendations based on the historical records stored in smart speakers and smart TVs.
Smart driving The Society of Automotive Engineers (SAE) denes six levels for autono- mous driving from L0 to L5 based on the degree of dependence the vehicle has on the driving system.
The L0-level vehicles need to reply on drivers operation completely, and the vehicles at level L3 and above allow the hands-off driving under certain circumstances, while the L5-level vehicles are completely operated by the driving system without a driver in all scenarios.
Currently only a handful of models of commercial passenger vehicle manu- facturers such as Audi A8, Tesla and Cadillac are equipped with L2 and L3 Advanced Driving Assistance System (ADAS). With the further enhancement of sensors and on-board processors, the year 2020 witnessed the emergence of more L3 models.
The vehicles with L4 and L5 autonomous driving system are expected to be rstly used in the commercial vehicle platforms in the enclosed industry parks.
But for the high-level autonomous driving on passenger vehicle platforms, it will require further optimization in technology, relevant policies and infrastruc- ture construction.
It is estimated that such passenger vehicles will not be put in use on the common roads until 2025.
As shown in Fig.
1.22, the AI development has undergone three stages and now AI is still in the stage of perceptual intelligence.
In the rst quarter of 2020, Huaweis all-scenario AI computing framework MindSpore was released to the open-source community.
Later Huawei released the GaussDB OLTP stand-alone database to the open-source community in June 2020, and released the server operating system to the open-source community on 31 December 2020.
Full-stack refers to a full-stack solution including chip, chip enable, training and reasoning framework and application enable.
All-scenario refers to an all-scenario deployment environment including public cloud, private cloud, all kinds of edge computing, IoT terminals and consumer terminals.
As the bedrock of Huaweis full-stack all-scenario AI solution, the Atlas articial intelligence computing solution, based on the Ascend AI processor, provides prod- ucts in different forms, including modules, circuit boards and servers to meet the all-scenario demands for computing power by customers.
Huaweis one-stop AI development platformModelArts ModelArts is a one-stop development platform that Huawei designed for AI developers.
It supports large-scale data preprocessing, semi-automatic labeling, distributed training, automated model building and on-demand model deploy- ment on end, edge and cloud, to help developers quickly build and deploy models and manage the full AI development lifecycle.
ModelArts features characteristics as follows.
(a) Automatic learning: With the automatic learning function, ModelArts can automatically design models, adjust parameters, train, compress and deploy models based on the labeled data, thus the developers do not need to have experience in coding or model development.
The automatic learning of Model Arts is mainly realized through ModelArts Pro, a professional development kit designed for enterprise- grade AI.
Based on the advanced algorithms and rapid training capability of HUAWEI CLOUD, ModelArts Pro provides the pre-installed workows and models to improve the efciency and reduce the difculty of AI application development by the enterprises.
It supports the users to recreate workow independently and the real-time development, sharing and launching of applications, conducive to building an open ecosystem through joint efforts, and the implementation of AI in industries that benet the general public.
The toolkit of ModelArts Pro includes the kit of natural language processing, text recognition, computer vision, etc., which will enable it to quickly respond to the demands of different industries and scenarios on AI implementation.
(b) Device-edge-cloud: Device, edge, and cloud refer to end-device, Huawei intelligent edge device, and Huawei CLOUD respectively.
(c) Support online inference: Online inference is an online service (Web service) that generates the real-time predictions upon each request.
(d) Support batch inference: batch inference is to generate a batch of predictions on a batch of data.
(e) Ascend AI processor: Ascend AI processor is an AI chip featuring high computing power and low power consumption designed by Huawei.
(f) High efciency of data preparation: ModelArts has a built-in AI data frame- work, which can enhance the efciency of data preparation through the convergence of automatic pre-labeling and hard example dataset labeling.
(g) Reduced training time: ModelArts is installed with Huaweis self-developed high-performance distributed framework MoXing, using core technologies including cascaded hybrid parallelism, gradient compression, and convolu- tion acceleration to speed up the model training by a large margin.
(h) ModelArts supports one-click deployment of models: ModelArts supports the deployment of models to end, edge, and cloud devices and scenarios by only one click, which can meet multiple requirements such as high concurrency and lightweight devices at the same time.
(i) Full-process management: ModelArts provides visual workow management of data, training, models and inference (covering the entire AI development cycle), and enables training auto-restart after power outage, training result comparison, and traceable management of models.
(j) Active AI market: ModelArts supports data and model sharing, which can help companies improve the efciency of internal AI development activities and can also let developers transform their knowledge into value.
MindSpore, All-Scenario AI Computing Framework Although the application of AI services to the device, edge and cloud scenarios is thriving in this intelligent age, AI technology still faces huge challenges including the high technological standards, soaring development costs and long deployment cycles.
These challenges are a brake on the development of AI ecosystem for developer in all-industry.
Consequently, the all-scenario AI com- puting framework MindSpore was introduced.
It was designed based on three principles: development-friendly, efcient execution and exible deployment.
In todays world of deep learning frameworks, if we call Googles TensorFlow, Amazons MXNet, Facebooks PyTorch and Microsofts CNTK as the four giants, then Huaweis MindSpore is the strongest competitor.
Thanks to the automatic parallelization provided by MindSpore, the senior data scientists and algorithm engineers dedicated to data modeling and problem solving can send an algorithm to visit dozens or even thousands of AI processing nodes with just several lines of code.
MindSpore supports architectures of different sizes and types, adaptable to all-scenario independent deployment, Ascend AI processor, and other processors such as GPUs and CPUs.
CANN Compute Architecture for Neural Networks (CANN) is a chip enablement layer Huawei built for deep neural networks and Ascend AI processors.
It consists of the following four major function modules.
(a) Fusion Engine: The operator-level fusion engine is mainly used to perform operator fusion to reduce the memory movement among operators and improve performance by 50%.
(b) CCE operator library: It is a deeply optimized common operator library of Huawei that can meet most of the needs of the mainstream computer vision and NLP neural network.
Certainly, it is inevitable for some clients and partners to ask for custom operators out of timeliness, privacy or doing research.
This will entail the third function module of CANN.
(c) Tensor Boost Engine (TBE). It is an efcient and high-performance custom operator development tool, which makes abstraction of hardware resources into application programming interfaces (API). The clients can quickly build the operators they need.
(d) The last module is the compiler at the bottom.
It provides ultimate optimiza- tion of performance to support Ascend AI processor in all scenarios.
Ascend AI processor Given the rising demands for AI, the AI processor market is currently monop- olized by a few companies, leading to high prices, long supply cycles and weak local service support.
The demands for AI in many industries have not been met effectively.
At the HUAWEI CONNECT conference in October 2018, Huawei released Ascend 310 and Ascend 910 processors specialized for AI inference and training scenarios.
The unique Da Vinci 3D Cube architecture of Ascend AI processors makes the series quite competitive in computing power, energy efciency and scalability.
Ascend 310 is a highly efcient AI system-on-chip (SoC) designed for the edge intelligent scenarios of inference.
It uses a 12 nm chip and delivers a computing power of up to 16 TOPS (tera operations per second) with a con- sumption of only 8 W, highly suitable for the edge intelligence scenarios requir- ing low power consumption.
Ascend 910 is currently the single chip with the greatest computing density, suitable for AI training.
It adopts a 7 nm chip and provides a computer power of up to 512 TOPS with a maximum power consumption of 350 W.
Atlas articial intelligence computing solutions Huawei Atlas articial intelligence computing solution is based on the Huawei Ascend AI processors to build an all-scenario AI infrastructure solution for device, edge and cloud scenarios through a wide range of products including modules, circuit boards, edge stations, servers and clusters, etc., as shown in the Fig.
As a crucial section of Huaweis full-stack all-scenario AI solution, Atlas launched its inference products in 2019 and brought the industry a complete AI computing solution by complementing the training products in 2020.
Mean- while, Huawei created a device-edge-cloud collaboration platform through all-scenario deployment, letting AI to empower every link in the industry.
The algorithmic bias is mainly cause by the biased data.
While we are making decisions with the help of AI, the algorithms may learn to discriminate against a certain group of individuals as trained on the collected data.
For instance, the algorithms could make discriminatory-prone decisions based on race, gender or other factors.
Even if we exclude the factors such as race or gender from the data, the algorithms could still make discriminatory decisions based on the personal information such as the name or address of a person.
Here is an example.
If you search with a name sounding like an African- American, you may get an advertisement for a tool of criminal records inquiry, which is unlikely to happen if you search with other styles of names.
Online advertisers tend to feed advertisements of a product with lower price to female viewers.
Google image app once mistakenly tagged a photo of black people as gorillas.
Currently, the existing AI algorithms are all data-driven, as the training of models require massive data.
While enjoying the convenience brought by AI, people are also threatened by the risk of privacy leakage.
For instance, the huge amount of user data that collected by some technology company may put us into the risk of full exposure of our daily life if these data are leaked.
When people are online, technically the technology companies can record every click, every page scroll, the viewing time spent for any content, and browsing history of the users.
These technology companies can also know the location of the users, where they have been, what they have done, and their education background, purchasing power, preferences and other personal privacy according to the users records of rides and purchases every day.
Along with the development of computer vision, it is more and more difcult to judge the credibility of images.
People can produce fake or manipulated images through image processors (e.g., Photoshop, PS), generative adversarial networks (GAN) and other techniques, making it really difcult to tell whether they are fake or real.
Lets take GAN as an example.
This concept was introduced by the machine learning researcher Ian Goodfellow in 2014.
In its name, G is for generative, which is quoted here to indicate that the model generates image-like information, rather than the predicted values related to the input data.
And AN is for adver- sarial network, as model uses two groups of neural networks that contest with each other like in a cat-and-mouse game, or like cashiers ghting banknote counterfeiters: the counterfeiter tries to deceive the cashier to believe that he is holding the real money, and the cashier tries to identify the authenticity.
Throughout the course of human development, people are always seeking ways to enhance efciency, namely, to harvest more with fewer resources.
We used sharp stones to hunt and gathered food more efciently, and invented steam engine to reduce the reliance on horses.
In the era of AI , AI will replace the jobs of high repetitiveness, low creativity and seldom social interactions, while the jobs of high creativity will not be easily replaced.
Easier Development Framework All the AI development frameworks are evolving to be simpler in operation while omnipotent in functions.
The threshold for AI development has been continu- ously lowered.
Algorithms and Models with Better Performance In computer vision, GAN is able to generate high-quality images that cannot be distinguished by the human eyes.
And the GAN-related algorithms have begun to be applied to other vision-related tasks, such as semantic segmentation, facial recognition, video synthesis and unsupervised clustering.
In natural language processing, major breakthroughs have been made in the Transformer-based pre-training models.
The relevant models such as BERT, GPT and XLNet have begun to be widely applied to industrial scenarios.
In reinforcement learning, AlphaStar of DeepMind defeated the top human players at the game StarCraft II.
Smaller Deep Models Models with better performance are often accompanied by larger parameters, and larger models will have to face the problem of operational efciency during industrial implementation.
Therefore, an increasing number of model compres- sion techniques have been proposed to further reduce the size and parameters of the models, accelerate the inference speed, and meet the requirements of indus- trial applications while ensuring the performance.
All-round development of the computing power at device, edge and cloud The application of articial intelligence chips to the cloud, edge devices and mobile terminals is expanding, further solving the problem of computing power for AI.
More Sophisticated AI Basic Data Services As the AI basic data service is becoming more mature, we will see more and more related data labeling platforms and tools being introduced to the market.
Safer Data Sharing On the premise of ensuring data privacy and security, federated learning makes use of different data sources to collaboratively train the models, so as to overcome the bottleneck of data as shown in Fig.
Huaweis global industry outlook report GIV 2020 (GIV 2025 for short) lists the 10 major development trends of intelligent technologies in the future.
Popularization of intelligent robots Huawei predicts that by 2025, 14% of the families across the globe will own a smart robot, which will play an important role in peoples daily life.
Popularization of AR/VR The report predicts that the percentage of the companies using VR/AR technology will reach 10% in the future.
The application of technologies including virtual reality will bring vigor and opportunities to the industries such as commercial presentation and audio-visual entertainment.
Application of AI in a wide range of elds It is predicted that 97% of the large enterprises will adopt AI technology, mainly exemplied by the employment of speech intelligence, image recogni- tion, facial recognition, human-computer interaction and so on.
Popularization of big data applications The enterprises will be making efcient use of 86% of the data they produce.
The Big data analysis and processing will save time and enhance efciency for the enterprises.
Weakening of search engine In the future, 90% of the people will have a smart personal assistant, which means that the chance for you to search something from a search portal will be greatly reduced.
Popularization of the Internet of Vehicles The cellular vehicle-to-everything (C-V2X) technology will be installed in 15% of the vehicles in the world.
Smart vehicles and cars on the Internet will be substantially popularized, providing a safer and more reliable driving experience.
Popularization of industrial robots Industrial robots will work side by side with people in manufacturing, with 103 robots for every 10,000 employees.
The hazardous, high-precision and high-intensity tasks will be assisted or completed by industrial independently.
robots 8.
Popularization of cloud technology and applications The usage rate of cloud-based applications will reach 85%. A majority of applications and program collaboration will be performed on the cloud.
Popularization of 5G Fifty-eight percent of the worlds population will enjoy 5G services.
We may anticipate a revolution of communications industry in the future, when the technology and speed of communications will be greatly advanced.
Popularization of digital economy and big data The amount of global storage data produced annually will reach as high as 180 ZB.
Digital economy and blockchain technology will be widely combined with the Internet.
This chapter introduces the basic concepts, development history and application background of AI.
By reading this chapter, the readers can nd that, as an interdis- ciplinary science, the application and development of articial intelligence will not be achieved without the support of other disciplines.
Its physical implementation is reliant on the large-scale hardware, and its upper-layer application is reliant upon software design and methods of implementation.
As learners, the readers are expected to understand the boundaries of the application of articial intelligence so as to ameliorate and improve themselves on this basis.
There are different interpretations of articial intelligence in different contexts.
Please elaborate on the articial intelligence in your eyes.
Articial intelligence, machine learning and deep learning are three concepts often mentioned together.
What is the relationship between them? What are the similarities and differences between the three terms? 3.
After reading the articial intelligence application scenarios in this chapter, please describe in detail a eld of AI application and its scenarios in real life based on your own life experience.
CANN is a chip enablement layer that Huawei introduced for deep neural networks and Ascend AI processors.
Please brief the four major modules of CANN.
Based on your current knowledge and understanding, please elaborate on the development trends of articial intelligence in the future in your view.
Machine learning is currently a mainstream research hotspot in the AI industry, entailing multiple disciplines such as probability theory, statistics, and convex optimization.
This chapter rst introduces the denition of learning in learning algorithms and the process of machine learning.
On this basis, it offers some commonly used machine learning algorithms.
Our readers will learn about some key concepts such as hyperparameters, gradient descent, and cross-validation.
Machine learning (including its branch deep learning) is the study of learning algorithms.
The so-called learning here refers to the situation that the perfor- mance of a computer program measured by performance metric P on a certain task T improves itself with experience E, then we call this computer program learning from experience E.
For instance, identifying junk email is a task T.
We can complete such tasks easily, because we have lots of experiences in doing so in daily life.
These experiences may come from daily emails, spam messages or even advertisements on TV.
We can summarize and conclude from these experiences that emails from unknown users that contain the words like discount and zero risk are more likely to be spam.
Referring to the learnt knowledge, we can distinguish whether an email that has never been read before is spam, as shown in Fig.
2.1a.
So, can we write a computer program to simulate the above process? As shown in Fig.
2.1b, we can prepare a number of e-mails, and pick out the junk emails by hands, as the experience E for this program.
However, the program cannot automatically summa- rize these experiences.
At this time, it is necessary to train the program through machine learning algorithms.
The computer program that has been trained is called a model, and in general the larger the number of emails is used for training, the better the model may be trained, and the larger the value of the performance metric P will be.
Spam identication is very difcult to achieve through traditional programming methods.
Theoretically, we should be able to nd a set of rules in line with the features of all junk emails, rather than that of the regular emails.
This method of using explicit programming to solve problems is known as a rule-based approach.
However, in practice, it is almost impossible to nd such a set of rules.
Therefore, to solve this problem, machine learning adopts the statistical-based method.
We can basically claim that machine learning is a method to let machines to learn rules automatically through samples.
Compared with using the rule-based method, machine learning can learn more complex rules and the rules that are difcult to describe, so as to handle more complicated tasks.
Machine learning is highly adaptable and can solve many problems in the AI eld, but this does not mean that machine learning will always be used preliminary in all cases.
As shown in Fig.
2.2, machine learning is suitable for problems which require a complex solution or involve a large amount of data, but the probability distribution of the data is unknown.
Machine learning can certainly solve the problem in other cases, but the cost is often higher than traditional methods.
Take the second quadrant shown in Fig.
2.2 as an example.
If the size of the problem is small enough to be solved by articial rules, then there is no need to use machine learning algorithms.
There are two main application scenarios in general for machine learning.
The rules are quite complicated or unable to be described, such as face recogni- tion and voice recognition.
The data distribution itself changes over time, and the program needs to be constantly re-adapted, such as predicting the trend of commodity sales.
The nature of machine learning algorithms is function tting.
Let f be the objective function, the purpose of the machine learning algorithm is to give a hypothetical function g that makes g(x) and f(x) as close as possible to the input x in any dened domain.
A simple example is the probability density estimation in statistics.
By the law of large numbers, the height of all Chinese population should follow a normal distribution.
Although the probability density function f of this normal distribution is unknown, we can estimate the mean and variance of the distribution by sampling, and then estimate f.
The relationship between the hypothesis function and the objective function is shown in Fig.
For a given task, we can collect a large amount of training data.
These data must t a given objective function f, otherwise, it is meaningless to learn such a task.
Next, the learning algorithm can analyze these training data and give a hypothetical function g that is similar to the objective function f as much as possible.
Therefore, the output of the learning algorithm is always not perfect and cannot be completely consistent with the objective function.
However, with the expansion of the training data, the degree of approximation of the hypothesis function g to the objective function f is gradually improved, and nally a satisfactory level of accu- racy can be achieved in machine learning.
It is worth mentioning that the objective function f is sometimes very abstract in existence.
For the classic image classication task, the objective function means the mapping from the image set to the category set.
In order to let a program to process logical information such as images and classes, it is necessary to use certain encoding methods to map images or categories one by one into scalars, vectors or matrices.
For example, you can number each category from 0, thereby mapping the class to a scalar.
You can also use different one-hot vectors to represent different classes, which is called one-hot encoding.
The encoding of image is a little more complicated, and is generally represented by a three-dimensional matrix.
With this encoding method, we can see the domain of denition the objective function f as a collection of three-dimensional matrices and its range as a collection of a series of serial numbers for classes.
Although the encoding process is not part of the learning algorithm, in some cases, the choice of encoding method will also affect the efciency of the learning algorithm.
Machine learning can deal with various types of problems, including the most typical ones such as classication, regression, and clustering.
Classication and regression are the two major types of prediction problems, taking up of 8090% of all the problems.
The main difference is that the output of classication is discrete serial numbers of classes (generally called as labels in machine learning), while the output of regression is continuous value.
The classication problem requires the program to indicate which of the k classes does the input belong to.
To solve this problem, machine learning algorithms usually output mapping from domain D to category labels 1, 2, . . ., k.
Image classication task is a typical classication problem.
In regression problems, the program needs to predict the output value for a given input.
The output of a machine learning algorithm is usually a mapping from the domain D to the real number domain R.
For instance, predicting the claim amount of the insured (used to set insurance premium), or predicting the price of securities in the future are all relevant cases.
In fact, classication problems can also be reduced to regression problems.
By predicting the probability of the image belonging to each class, the machine learning can obtain the result of the classication.
The clustering problem needs to divide the data into multiple categories according to the inherent similarity of the data.
Unlike the classication problem, the dataset of the clustering problem does not contain manually labeled labels.
The clustering algorithm makes the data similar to each other within the class as much as possible, while the data similarity between the classes is relatively small, so as to implement classication.
Clustering algorithms can be used in scenarios like image retrieval, user portrait generation and etc.
According to whether the training dataset contains manually tagged labels, machine learning can be generally divided into two typessupervised learning and unsupervised learning.
Sometimes, in order to distinguish it from unsupervised learning, supervised learning is also called learning under supervision.
If some data in the dataset contains labels and the majority of the data does not, then this learning algorithm is called semi-supervised learning.
Reinforcement learning mainly focuses on multi-step decision-making problems, and automatically collects data for learning in the interaction with the environment.
Generally speaking, supervised learning is allowing the computer to compare stan- dard answers when it is trained to answer the multiple-choice questions.
The computer tries its best to adjust its model parameters, expecting that the inferred answer is as consistent as possible with the standard answer, and nally learn how to solve the question.
Using samples of known labels, supervised learning can train an optimal model meeting the required performance.
Using this model, any input can be mapped to the corresponding output, so as to predict the unknown data.
Figure 2.4 shows a supervised learning algorithm that is highly simplied.
The features in the graph can be understood as data items.
Although this interpretation is not sufcient to some extent, it will not affect our description on this supervised learning algorithm.
The supervised learning algorithm takes features as input and the predicted value of the targets as output.
Figure 2.5 shows a practical example.
In this example, we hope to make an overall prediction that whether a user enjoys exercises by taking weather conditions into account.
Each row in the table is a set of training examples that records the weather characteristics of a specic day and the users enjoyment of exercises.
Similar algorithms can be applied to other scenarios such as product recommendations.
The input (feature) and output (target) of a supervised learning algorithm can be either continuous or discrete.
When the value of the target variable is continuous, the output of the supervised learning algorithm is called a regression model.
The regression model reects the features of the attribute values in the sample dataset, and expresses the relationship of the sample mapping through functions to show the dependency between the attribute values.
The attribute value mentioned here includes feature and target.
Regression model is widely used in time series forecast- ing.
For instance, how much money can you earn form stocks next week? What will the temperature be tomorrow? So on and so forth.
Correspondingly, when the target variable takes discrete values, the output of the supervised learning algorithm is called a classication model.
Through the classication model, the samples in the sample dataset can be mapped to the given classes.
Such as whether there will be trafc jams on a highway during the morning rush hour tomorrow? Which one is more attractive to customers, a ve-yuan voucher or a 25%-off discount? Etc.
Although the range of the regression model can be an innite set, the output of the classication model is often nite.
This is because the size of the dataset cannot grow indenitely, and the number of classes in the dataset should to the most be the same with the number of training examples.
Therefore, the number of classes is not innite.
When training a classication model, an articially designated class set L is often needed for the model to select category output.
The size of the set L is generally recorded as K, which is the number of possible classes.
Compared with supervised learning, unsupervised learning is like letting the com- puter do multiple-choice questions without telling it what the correct answer is.
In this case, it is difcult for the computer to secure the correct answer.
But by analyzing the relationship between these questions, the computer can classify the questions so that the multiple-choice questions in each class have the same answer.
The unsupervised learning algorithm does not require labeling samples, but directly modeling the input dataset, as shown in Fig.
Clustering algorithm is a typical unsupervised learning algorithm that can be summarized by the proverb: birds of a feather ock together.
The algorithm only puts the samples of high similarity together.
For newly input samples, it only needs to calculate their similarity with the existing samples, and then classify them according to the degree of similarity.
Biologists have used the concept of clustering to study the interspecies relationship for a long time.
As shown in Fig.
2.7, after classifying the iris owers based on the size of sepal and petal, the iris has been divided into three categories.
Through the clustering model, the samples in the sample dataset can be divided into several categories, making the similarity of samples under the same category relatively higher.
The scenarios of application for the clustering model include that what kinds of audiences like to watch movies of the same subject, and which components are damaged similarly, etc.
Semi-supervised learning is a combination of supervised learning and unsupervised learning.
This algorithm attempts to allow the learner to automatically utilize a large amount of unlabeled data to assist the learning of a small amount of labeled data.
Traditional supervised learning algorithms need to learn from a large number of labeled training samples to build a model for predicting the labels of new samples.
For example, in a classication task, the label suggests the class of the sample.
And in a regression task, the label suggests the real-valued output of the sample.
With the rapid development of humans ability to collect and store data, in many practical tasks, it is very easy to acquire a large amount of unlabeled data, and labeling them often consumes a lot of efforts and materials.
For example, for webpage recommen- dation, users are required to mark web pages they interest in.
But few users are willing to spend a lot of time to mark, so the web pages with marked information are limited.
But there are countless web pages that are not marked, or marked, which can be used as unmarked data.
As shown in Fig.
2.8, semi-supervised learning does not require manual labeling on all the samples like supervised learning, nor is it completely independent from the target like unsupervised learning.
In the semi-supervised learning datasets, generally speaking, there are only a few samples labeled.
Taking the iris classication problem presented in Fig.
2.7 as an example.
A small amount of supervised information is added to the dataset, as shown in Fig.
Lets assume the circle represents the Setosa sample, the triangle represents the Versicolor sample, the square represents the Virginica sample, and the star represents the unknown sample.
The clustering algorithm has been introduced in unsupervised learning, suppose its output is shown by the dotted circle in Fig.
Counting the circles including the highest number of known samples and then this class can be used as the class for this cluster.
To be more specic, the upper-left cluster belongs to Setosa, while the upper right cluster obviously belongs to Virginica.
By combining unsupervised algorithm and super- vised information, semi-supervised algorithm can achieve higher accuracy with lower labor costs.
Reinforcement learning is mainly used to solve multi-step decision-making prob- lems, such as Go game, video games, and visual navigation.
Unlike the problems studied by supervised learning and unsupervised learning, these problems are often difcult to nd accurate answers.
Taking Go as an example.
It takes about 10,170 operations to exhaust the results of the game (there are only 1080 atoms in the to nd the universe). So, for a given and common situation, perfect move.
is difcult it Another characteristic of the multi-step decision problem is that it is easy to dene a reward function to evaluate whether the task has been completed.
The reward function of Go can be dened as whether to win the game the reward function of electronic games can be dened as the score.
The goal of reinforcement learning is to nd an action strategy to maximize the value of the reward function.
As shown in Fig.
2.10, the two most important parts of a reinforcement learning algorithm are the model and the environment.
In different environments, the model can determine its own actions, and different actions may have different effects on the environment.
Still, in the case of solving test questions, the computer can give the answer randomly, and the teacher will give a score based on the answer given.
But if the situation is only limited to this case, it is impossible for the computer to learn how to solve the question, because the teachers grading does not contribute to the training process.
In this case, the importance of status and rewards and punishments are highlighted.
A higher test score can make the teacher satised and then give the computer a certain reward.
On the contrary, a lower test score may incur penalties.
As a motivated computer, it is bound to hope that by adjusting its own model parameters, it can get more rewards because of its answers.
In this process, no one provides training data for the learning algorithm or tells the reinforcement learning system how to make the correct move.
All data and reward signals are dynamically generated during the interaction between the model and the environment and are automatically and dynamically learned.
No matter it is good behavior or bad behavior, it can help the model to learn.
A complete machine learning project often involves data collection, data cleaning, feature extraction and selection, model training, model evaluation and testing, model deployment and integration, as shown in Fig.
This section introduces the concepts related to data collection and data cleaning, which are fundamental to understand what is feature selection.
After selecting reasonable features, it is neces- sary to train and evaluate the model based on these features.
This is not a one-kick process, but requires constant feedback and iteration to harvest satisfactory results.
At last, the model needs to be deployed to the specic application scenarios to put theories into practice.
A dataset is a set of data used in a machine learning project, and each data is called a sample.
The items or attributes that reect the performance or nature of the sample in a certain aspect are called features.
The dataset used in the training process is called the training set, and each sample is called a training sample.
Learning (training) is the process of learning a model from data.
The process of using the model to make predictions is called testing, and the dataset used for testing is known as the test set.
Each sample in the test set is called a test sample.
Figure 2.12 shows a typical dataset.
In this dataset, each row indicates a sample, and each column refers to a feature or label.
When the task is determined, such as predicting housing prices based on oor area, school district, and house orientation, the features and labels are also determined.
Therefore, the row and column headers of the dataset should remain unchanged throughout the machine learning project.
The training set and the test set are relatively free to split, and the researcher can determine which samples belong to the training set based on experience.
A too low proportion of the test set may result in randomness of model testing, not able to properly evaluate the performance of the model.
While a high proportion of the training set may result in low sample utilization and the model cannot learn thor- oughly.
Therefore, the common ratio between training set and dataset is that the training set accounts for 80% of the total number of samples, and the test set accounts for 20%. In this example, there are four samples in the training set and one sample in the test set.
Data is vital to the model and determines the limit of the models capabilities.
Without good data, there will be no good models.
However, data quality is a commonly problem bothering real data, as shown in Fig.
Following are some typical problems on data quality.
Incomplete: Data lacks attributes or containing missing values.
Noisy: Data contains erroneous records or outliers.
Inconsistent: Data contains conicting records or discrepancies.
Such data is called dirty data.
The process of lling in missing values, nding and eliminating data abnormalities In addition, data preprocessing often involves data dimensionality reduction and data standardization.
The purpose of data dimensionality reduction is to simplify data attributes and avoid dimensional explosion while the purpose of data standardization is to unify the dimensions of each feature, thereby reducing the difculty of training.
The content of data dimensionality reduction and data standardization will be introduced in detail in the later passages, and this section only talks about data cleaning.
is called data cleaning.
What are handled by the machine learning model are all features.
The so-called feature is the numerical representation of the input variable that can be used in the the collected data can be used by the algorithm after model.
In most cases, preprocessing.
The preprocessing operation mainly includes the following procedures.
Data ltering.
Handling missing data.
Handling possible errors or outliers.
Combining data from multiple sources.
Data aggregation.
The workload of data cleaning is often quite heavy.
Research shows that data scientists spend 60% of their time on cleaning and organizing data in machine learning researches, as shown in Fig.
On the one hand, this shows how difcult data cleaning is, and that the data collection featuring different methods and contents will require different methods for data cleaning.
What is more, it also shows that data cleaning plays a crucial role in subsequent model training and optimization.
Another important role is that the more thoroughly the data is cleaned, the less likely the model is to be affected by abnormal data, thus ensuring the models training performance.
Usually, there are many different features in a dataset, some of which may be redundant or unrelated to the target.
For example, when predicting housing prices based on oor area, school district, and daily temperature, the daily temperature is apparently an irrelevant feature.
Through feature selection, these redundant or irrelevant features can be eliminated, so that the model is simplied and easier to be interpreted by users.
At the same time, feature selection can also effectively reduce the time of model training, avoid dimensional explosion, improve the gen- eralization performance of the model, and avoid overtting.
Common methods for feature selection include lter methods, wrapper methods, and embedded methods, which will be introduced successively in the following passages.
The lter method is independent when selecting features and has nothing to do with the model itself.
By measuring the correlation between each feature and the target attribute, lter method applies a statistical measurement to score each feature.
By sorting these features on the basis of the scores, you can decide to keep or eliminate specic features.
Figure 2.15 shows the machine learning process using lter methods.
Statistical measures commonly used in ltering include Pearsons correlation coefcient, Chi-Square coefcient, and mutual information.
Since lter does not consider the relationship between features, it is only prone to select redundant variables.
The wrapper method uses a predictive model to score a subset of features, and considers the feature selection problem as a search problem, where the wrapper will evaluate and compare different feature combinations, and the predictive model will be used as a tool for evaluating feature combinations.
The higher the accuracy of the prediction model, the more the feature combination should be retained.
Figure 2.16 displays the machine learning using the wrapper method.
One of the popular wrapper methods is recursive feature elimination (RFE). Wrapper methods usually provide the best-performing feature set for a specic class of model, but it needs to train a new model for each feature subset, so the amount of operations is extensive.
The embedded method uses feature selection as part of model building, as shown in Fig.
Unlike the lter and wrapper method, the model using the embedded method actively learns how to perform feature selection during training.
The most common embedded feature selection method is regularization.
Regularization is also called the penalty method.
By introducing additional constraints when optimizing the prediction algorithm, the complexity of the model is reduced, namely, the number of features is reduced.
Common regularization methods include ridge regression and Lasso regression.
After nishing data cleaning and feature extraction, it is time to build the model.
Taking supervised learning as an example, model construction generally follows the steps shown in Fig.
The core of model construction is model training, veri- cation and testing.
This section briey explains the training and prediction process using one simple example.
More details will be introduced in the following chapters.
In the example of this section, we need to use a classication model to determine whether someone needs to change suppliers under certain features.
Assuming that Fig.
2.19 shows the cleaned dataset, the task of the model is to predict the target as accurately as possible on the basis of the known features.
During the training process, the model can learn the mapping relationship between features and targets based on the samples in the training set.
After training, we can get the following model: The output of the model is the probability of truth value of the target.
We know that as the training data increases, the accuracy of the model will also increase accordingly.
So why not use all the data for training, instead of only taking a part of it as the test set? This is because that the performance of the model in the face of unknown data, not the known data, is what we should look at.
The training set is like an exam bank that students read through while preparing for an examination.
It is not a surprising thing no matter how high the accuracy rate will be for the students, because the exam bank always has a limitation.
As long as they have a good memory, the students can even memorize all the answers after all.
Only the formal examination can really test the students acquisition of knowledge, because the questions in the ofcial examinations may be something they have never seen before.
The test set is equivalent to an examination prepared by the researchers for the model.
In other words, in the entire dataset (including training set and test set), the model has the right to consult only the features of the training set and test set.
The target of the test set can only be used by the researchers when evaluating the performance of the model.
What is a good model? The most important evaluation indicator is the models generalization ability, also known as the prediction accuracy of the model dealing with actual business data.
There are also some engineering indicators that can be used to evaluate the model: interpretability, which describes the degree of straight- forwardness of the models prediction results prediction rate, which refers to the average time it takes for the model to predict each sample plasticity, which refers to the acceptability of model prediction rate in actual business process as the business volume expands.
The goal of machine learning is to make the learned model applicable to new samples, not just on training samples.
The ability of the learned model to apply to new samples is called generalization ability, also addressed as robustness.
The difference between the predicted result of the learned model on the sample and the true result of the sample is called error.
The training error refers to the error of the model on the training set, and the generalization error refers to the error of the model on the new sample (test set). Obviously, we want to have a model with smaller generalization error.
Once the model is formed and xed, all possible functions will construct a space, which is called hypothesis space.
The machine learning algorithm can be seen as an algorithm that searches for a suitable tting function in the hypothesis space.
A mathematical model that is too simple, or the training time is too short, will cause an increasing training error for the model.
This phenomenon is called undertting.
For the former, it should use a more complex model for retraining for the latter, it only needs to extend the time to effectively eliminate undertting.
However, to accurately determine the cause of under-tting often requires certain experience and methods.
On the contrary, if the model is too complex, it may lead to a small training error, but a weaker generalization ability, which means a larger generalization error known as overtting.
There are many methods to reduce over-tting.
The common ones include appropriately simplifying the model, ending training before the over-tting occurs, and using dropout and weight decay.
Figure 2.20 shows the results of undertting, good tting and overtting for the same dataset.
The capacity of a model refers to its ability to t a variety of functions, also known as the complexity of a model.
When the capacity is compatible for the complexity of the task and the amount of training data provided, the algorithm will usually have the best performance.
A model with insufcient capacity cannot thus undertting may be provoked.
As shown in handle the complex tasks, Fig.
2.20a, the data distribution is in a shape of hook, but the model is linear and cannot describe the data distribution properly.
A model with a high capacity can handle complex tasks, but when the capacity surpasses the level that the task needs, overtting may be provoked.
As shown in Fig.
2.20c, the model tries to t the data with a very complex function.
Although the training error is reduced, it can be inferred that such a model cannot predict the target value of a new sample properly.
The effective capacity of the model is limited by algorithms, parameters, and regularization methods.
Generally speaking, the generalization error can be interpreted as: Total error Bias 2 + Variance + Unresolvable error Among them, bias and variance are two sub-forms that we need to pay attention to.
As shown in Fig.
2.21, the Variance is the degree of deviation of the models prediction results near the mean, which is an error derived from the sensitivity of the model to small uctuations on the training set.
Bias is the difference between the average value of the models prediction results and the correct value we are trying to predict.
The unresolvable error refers to the error caused by the imperfection of the model and the niteness of the data.
In theory, if there is an innite amount of data and a perfect model, the so-called unresolvable errors can be resolved.
But in fact, it is impossible to realize, so the generalization error can never be eliminated.
Ideally, we want to choose a model that can accurately capture the laws in the training data and can also summarize the invisible data (the so-called new data).
However, generally speaking, it is impossible for us to accomplish these two things at the same time.
As shown in Fig.
2.22, as the complexity of the model increases, the training error gradually decreases.
At the same time, the test error will decrease to a certain point as the complexity increases, and then increase in the opposite direction, forming a concave curve.
The lowest point of the test error curve suggests the ideal level of model complexity.
When measuring the performance of the regression model, commonly used indicators include mean absolute error (MAE), mean square error (MSE), and correlation coefcient R.
Assuming that the true target values of the test example are y1, y2,. . ., ym, and the corresponding predicted values are by, by, , by, then the denition of the above indicators is as follows: MAE X y by j j m MSE m y by X R 1 RSS TSS 1 P P y by y y Where, TSS represents the difference between the sample values, and RSS repre- sents the difference between the predicted value and the sample value.
The values of the MAE and MSE indicators are both non-negative, and the closer to 0, the better the performance of the model.
The value of R2 is not greater than 1, and the closer to 1, the better the performance of the model.
When evaluating the performance of a classication model, a method called confusion matrix is often used, as shown in Fig.
The confusion matrix is a k-dimensional square matrix, where k represents the number of all categories.
The value in the i-th row and the j-th column in Fig.
2.23 represents the number of samples that are actually the i-th type but are judged to be the j-th type by the model.
Ideally, for a classier with higher accuracy, most of the examples should be represented by the diagonal of the confusion matrix, while other values are 0 or close to 0.
For the two-classier confusion matrix shown in Fig.
2.23, the denition of each symbol is as follows.
Positive tuple P: tuple of the major classes of interest.
Negative tuple N: other tuples except P.
True positive example TP: positive tuples correctly classied by the classier.
True negative example TN: the negative tuple correctly classied by the classier.
False positive example FP: a negative tuple that is incorrectly marked as a positive tuple.
False negative example FN: A positive tuple that is incorrectly marked as a negative tuple.
Figure 2.24 shows the rest concepts in the binary classier confusion matrix.
Here let us cite the example of document retrieval to clarify the concepts of precision and recall.
The precision describes the proportion of documents that are truly related to the subject among all the documents retrieved.
The recall describes the retrieved documents related to the search subject, and the proportion of all related documents in the library.
At the end of this section, lets take an example to illustrate the calculation of the confusion matrix of the binary classiers.
Assuming that a classier can identify whether there is a cat in an image and 200 images are now used to verify the performance of this model.
Among them, 170 are labeled as images with cats and 30 are labeled not.
The performance of the model is as shown in Fig.
It can be seen that the recognition result of the model is that 160 images are marked with cats and 40 pictures not.
It can be calculated that is 140/160 87.5%, the recall is 140/170 82.4%, and the accuracy is (140 + 10)/ 200 75%.
the precision of the model Parameters, as part of what the model has learned from historical training data, are the key to machine learning algorithms.
Generally speaking, the model parameters are not manually set by the researchers, but are obtained by data estimation or data learning.
Identifying the parameter values of the model is equivalent to dening the function of the model, so the model parameters are usually saved as part of the learning model.
When implementing model predictions, parameters are also an indispensable component.
Examples of model parameters include weights in arti- cial neural networks, support vectors in support vector machines, and coefcients in linear regression or logistic regression.
There are not only parameters but also hyperparameters in the model.
Different from parameters, hyperparameters are external congurations of the model and are often used in the process of estimating model parameters.
The most fundamental difference between the two is that the parameters are automatically learned by the model, while the hyperparameters are manually engineered.
When handling differ- ent prediction modeling problems, the it model hyperparameters.
In addition to being directly specied by the researcher, model hyperparameters can also be set using heuristic methods.
Common model hyperparameters include the penalty coefcient in Lasso or ridge regression, the learning rate, number of iterations, batch size, activation function, and number of neurons in the training neural network, the C and of the support vector machine, and the K in KNN, the number of decision tree models in the random forest, etc.
is usually necessary to adjust Model training generally refers to optimizing model parameters, and this process is completed by a gradient descent algorithm.
According to the training effect of the model, a series of hyperparameter search algorithms can be used to optimize the hyperparameters of the model.
This section rst introduces the gradient descent algorithm, and then the concept of the validation set, and then elaborates on the hyperparameter search algorithm and cross-validation.
The optimization idea of the gradient descent algorithm is to use the negative gradient direction of the current position as the search direction, which is the fastest descent direction of the current position, as shown in Fig.
2.26a.
The formula for gradient descent is as follows: w w f x Where, is called the learning rate, and w represents the parameters of the model.
As w gets closer to the target value, the amount of change in w gradually decreases.
When the value of the objective function barely changes or reaches the maximum number of iterations of gradient descent, then it reaches algorithm convergence.
It is worth noting that when using the gradient descent algorithm to nd the minimum value of a non-convex function, different initial values may lead to different results, as shown in Fig.
2.26b.
When applying gradient descent to model training, multiple variants can be used.
Batch Gradient Descent (BGD) uses the gradient mean of the samples in all datasets at the current point to update the weight parameters.
Stochastic Gradient Descent (SGD) randomly selects a sample in a dataset, and updates the weight parameters through the gradient of this sample.
Mini-batch Gradient Descent (MBGD) com- bines the characteristics of BGD and SGD, and selects the gradient mean of n samples in the dataset to update the weight parameters each time.
Figure 2.27 shows the different performances of the three variants of gradient descent.
Among them, the bottom-up curve corresponds to BGD, the top-down curve corresponds to SGD, and the right-to-left curve corresponds to MBGD.
BGD is the most stable at runtime, but because every update needs to traverse all samples, it consumes a lot of computing resources.
Each update of SGD randomly selects samples, although it improves the computational efciency, it also brings instability, which may cause the loss function to produce turbulence or even reverse displacement during the process of dropping to the lowest point.
MBGD is a method after SGD and BGD are balanced, and it is also the most commonly used gradient descent algorithm in machine learning.
The training set is a collection of samples used in model training.
During the training process, the gradient descent algorithm will try to improve the models prediction accuracy for the samples in the training set.
This causes the model to perform better on the training set than on the unknown dataset.
In order to measure the generaliza- tion ability of the model, people often randomly select a part of the entire dataset as a test set before training, as shown in Fig.
The samples in the test set are not involved in training, so they are unknown to the model.
It can be approximated that the performance of the model on the test set is the performance of the model under unknown samples.
The optimization goal of hyperparameters is to improve the generalization ability of the model.
The most intuitive idea is to try different hyperparameter values, evaluate the performance of these models on the test set, and select the model with the strongest generalization ability.
The problem is that the test set cannot participate in model training in any form, even for hyperparameter search.
Therefore, some samples should be randomly selected from the training set, and the set of these samples is called the validation set.
The samples of the validation set also do not participate in training, and are only used to verify the effect of hyperparameters.
Generally speaking, the model needs to be optimized repeatedly on the training set and validation set to nally determine the parameters and hyperparameters and be evaluated on the test set.
Methods commonly used to search model hyperparameters include grid search, random search, heuristic intelligent search, and Bayesian search.
Grid search attempts to exhaustively search for all possible hyperparameter combinations to form a grid of hyperparameter values, as shown in Fig.
2.29a.
In practice, the range and step length of the grid often need to be manually designated.
In the case of a relatively small number of hyperparameters, grid search is applicable, so grid search is feasible in general machine learning algorithms.
However, in the case of neural networks, grid search is too expensive and time-consuming, so it is not adopted in most cases.
In the case of a large hyperparameter search space, the result of using random search will be better than grid search, as shown in Fig.
2.29b.
Random search implements random sampling of parameters, where each setting is to sample from the distribution of possible parameter values, trying to nd the best subset of parameters.
To use random search, you need to coarse adjustment rst and then ne adjustment.
Namely, searching in a coarse range rst, and then narrowing the search range according to the position where the best result appears.
It is worth noticing that some hyperparameters may be more important than others in actual operation.
In this case, the most important hyperparameters will directly affect the search bias, while the secondary hyperparameters may not be well optimized.
The above-mentioned method of dividing the verication set has two main prob- lems: the chance of sample division is great, and the verication result is not convincing and the number of samples that can be used for model training is further reduced.
In order to solve this problem, the training set can be divided into k groups for k-fold cross-validation.
K-fold cross-validation will perform k rounds of training and verication, where one set of data is used as the verication set in turn, and the remaining k 1 sets of data are used as the training set.
This will get k models and their classication accuracy on the validation set.
The average of these k classication accuracy rates can be used as a performance indicator for the generalization ability of the model.
K-fold cross-validation can avoid contingency in the process of dividing the validation set, and the validation results are more convincing.
However, using k- fold cross-validation requires training k models.
If the dataset is large, the training will be time-consuming.
Therefore, k-fold cross-validation is generally applicable to smaller datasets.
The k value in k-fold cross-validation is also a hyperparameter, which needs to be determined through experiments.
In an extreme case, the value of k is the same as the number of samples in the training set.
This approach is called leave-one-out cross- validation, because one training sample is left as a validation set during each training.
The training result of leaving-one-out cross-validation is better, because almost all training samples are involved in the training.
But leaving-one-out cross- validation takes a longer time, so it is only suitable for small dataset.
As shown in Fig.
2.30, there are many common algorithms for machine learning, and a detailed introduction of these algorithms may take a long as a whole book.
Therefore, this section only briey introduces the principles and basic ideas of these algorithms.
Readers who are interested in this topic can refer to other books for in-depth understanding.
Linear regression is a statistical analysis method that uses regression analysis in mathematical statistics to determine the quantitative relationship between two or more variables.
It belongs to supervised learning.
As shown in Fig.
2.31, the model function of linear regression is a hyperplane: h x wx b Where, w is the weight parameter, b is the bias, and x is the sample.
The relationship between the predicted value of the model and the true value is as follows: y h x Where, y represents the true value and represents the error.
The error is affected by many factors.
According to the central limit theorem, the error obeys the normal distribution.
N 0, Where, the probability distribution of the true value can be obtained.
y N h x , According to the maximum likelihood estimation, the goal of model optimization is arg max Y P Y y X x j arg max Y p exp h x y Where, argmax represents the maximum point, which is h that maximizes the value is a constant that has of the objective function.
In the objective function, nothing to do with h, and multiplying or dividing the objective function by a constant will not change the position of the maximum point, so the optimization objective of the model can be transformed into p exp Y h x y arg max Because the logarithmic function is monotonic, taking ln for the objective function will not affect the maximum point.
arg max ln exp Y h x y X arg max h x y By taking the negative of the objective function, the original maximum point will become the minimum point.
At the same time, we can also multiply the objective function by a constant to convert the optimization goal of the model into arg min 2m X h x y Obviously, the loss function is J w 2m X h x y We hope that the predicted value is as close as possible to the true value, that is, to minimize the loss value.
The method of gradient descent can be used to nd the weight parameter w when the loss function is minimized, and then the model construction is completed.
Polynomial regression is a branch of linear regression.
Generally, the complexity of the dataset would exceed the possibility of tting with a straight line, that is, using the original linear regression model will obviously undert.
The solution is to use polynomial regression, as shown in Fig.
2.32, the formula is h x wx wx wx b Where, n represents the polynomial regression dimension.
The polynomial regression dimension is a hyperparameter.
If you choose it carelessly, it may cause overtting.
Applying regularization helps reduce overtting.
The most common regularization method is to add a square sum loss on top of the objective function h x wx wx wx b Where kk represents the L2 regular term.
The linear regression model using this loss function is also called the ridge regression model.
Similarly, the linear regres- sion model with the added absolute value loss is called the Lasso regression model, and its formula is J w 2m X h x y X wk k Where kk represents the L1 regular term.
Logistic regression model is a classication model used to solve classication problems.
The denition of the model is as follows: h x P Y 1 Xj g wx b Where g represents the sigmoid function, w represents the weight, and b, the bias.
In the formula, is a linear function of x, so logistic regression, like linear regression, belongs to the generalized linear model.
The denition of the sigmoid function is as follows: g x 1 exp xf g The image of the sigmoid function is shown in Fig.
By comparing the magnitude relationship between P(Y 1X) and the threshold t, the classication result corresponding to x can be obtained.
The threshold t here is a hyperparameter of the model, which can be chosen arbitrarily.
It can be seen that when the threshold is large, the model tends to judge the sample as a negative example, so the precision rate will be higher when the threshold is smaller, the model tends to judge the sample as a positive example, so the recall rate will be higher.
Generally, 0.5 can be used as the threshold.
According to the idea of maximum likelihood estimation, when the sample is a positive example, we expect P(Y 1X) to be larger when the sample is a negative example, we expect P(Y 0X) to be larger.
In other words, we expect the following equation to be as large as possible whatever the sample is: P P Y 1 Xj P Y 0 Xj Replace P(Y 1X) and P(Y 0X) with h(x) to get P h x 1 h x Therefore, the goal of model optimization is arg max Y P arg max Y h x 1 h x The derivation process similar to linear regression can take the logarithm of the objective function without changing the position of the maximum point.
Therefore, the optimization goal of the model is equivalent to arg max X y ln h x 1 y ln 1 h x Multiplying the objective function by the constant 1/m will cause the original maximum point to become the minimum value point, which is arg min 1 m X y ln h x 1 y ln 1 h x Therefore, the loss function of logistic regression is J w X m y ln h x 1 y ln 1 h x Where, w represents the weight parameter, m is the number of samples, x is the sample, and y is the true value.
The value of the weight parameter w can also be obtained through the gradient descent algorithm.
Softmax regression is a generalization of logistic regression, which is applicable for k classication problems.
Essentially, the softmax function compresses (maps) a k-dimensional arbitrary real number vector into another k-dimensional real number vector to represent the probability distribution of the category of the sample.
The softmax regression probability density function is as follows: P Y c xj P exp w x b exp w x b As shown in Fig.
2.34, the softmax function assigns probability values to each category in the multi-class problem, and these probabilities add up to 1.
Among these categories, the probability value of the sample category being apple is the largest, which is 0.68, so the predicted value of the sample should be the apple.
Decision tree is a tree structure (binary or non-binary) classier, as shown in Fig.
Each non-leaf node represents a test on a feature attribute, each branch represents the output of this feature attribute in a certain value range, and each leaf node stores a category.
The process of using a decision tree to make a decision is to start from the root node, test the corresponding feature attributes in the items to be classied, and select the output branch according to its value until the leaf node is reached, and the category stored in the leaf node is used as the decision result.
The most important thing in the decision tree model is the structure of the tree.
The construction of the so-called decision tree is to select attributes to determine the topological structure between each feature attribute.
The key step in constructing a decision tree is to perform the division operation according to all the feature attributes, compare the purity of the result set of all the division operations, and select the attribute with the highest purity as the data point of the split dataset.
The learning algorithm of the decision tree is the algorithm that constructs the decision tree, and the commonly used algorithms include ID3, C4.5 and CART.
The differ- ence between these algorithms is mainly in the quantitative indicators of purity, such as information entropy and Gini coefcient: H X X p log p Gini 1 X p Where, pk represents the probability that the sample belongs to class k, and K represents the total number of categories.
The greater the difference in purity before and after segmentation, the more conducive that judging a certain feature is to the improvement of the accuracy of the model, indicating that it should be added to the decision tree model.
In general, the process of building a decision tree consists of the following three stages.
Feature selection: select a feature from the features of the training data as the split criterion for the current node (different criteria for feature selection produce different decision tree algorithms).
Decision tree generation: According to the selected feature evaluation criteria, child nodes are generated recursively from top to bottom until the dataset is inseparable, then the decision tree growth is stopped.
Pruning: By reducing the size of the tree to suppress the overtting of the model, it can be divided into pre-pruning and post-pruning.
Figure 2.36 shows a case of classication using a decision tree model.
The classi- cation result is affected by three attributes: tax refund, marital status and taxable income.
From this example, we can see that the decision tree model can not only handle the case where the attribute takes two values, but also the case where the attribute takes multiple values or even continuous values.
In addition, the decision tree model is interpretable, and we can intuitively analyze the importance relation- ship between attributes based on the structure chart shown in Fig.
2.36b.
Support vector machine (SVM) is a linear classier with the largest interval dened in the feature space.
The learning algorithm of SVM is an optimal algorithm for solving convex quadratic linear programming.
In summary, the core concepts of SVM include the following two aspects.
Search for the optimal hyperplane in the feature space based on the structural risk minimization theory, so that the learner obtains global optimization, and the expectation in the entire sample space satises a certain upper bound with a certain probability.
For linearly inseparable data, map the linearly inseparable samples of the low-dimensional input space to the high-dimensional feature space to make them linearly separable based on a nonlinear mapping algorithm, so that the high-dimensional feature space adopts the linear algorithm for the nonlinearity of the sample Linear analysis of features becomes possible.
Straight lines are used to divide the data into different categories, but in fact we can nd multiple straight lines to separate the data, as shown in Fig.
The core idea of SVM is to nd a straight line that meets the above conditions, and make the points closest to the straight line as distant as possible from this straight line.
This will give the model a strong generalization ability.
These points closest to the straight line are called support vectors.
Linear SVM can perform properly on linear separable datasets, but we cannot use straight lines to divide non-linear datasets.
At this time, a kernel function is needed to construct a nonlinear SVM.
The kernel function allows the algorithm to t the feature space, as shown in hyperplane in the transformed high-dimensional Fig.
Common kernel functions include linear kernel function, polynomial kernel function, Sigmoid kernel function and Gaussian kernel function.
The Gauss- ian kernel function can map samples to innite dimensional space, so the effect is also better, and it is one of the most commonly used kernel functions.
The K-nearest neighbor (KNN) algorithm is a theoretically mature method and one of the simplest machine learning algorithms.
The KNN algorithm is a non-parametric method, which tends to perform better in datasets with irregular decision boundaries.
The idea of this method is: if most of the K nearest samples (i.e., nearest neighbors in the feature space) of a sample in the feature space belong to a certain category, then the sample also belongs to this category.
The core concept of the KNN algorithm is Whats around cinnabar goes red, and whats around ink turns black, featuring a concise logic.
But like k-fold cross- validation, K in the KNN algorithm is also a hyperparameter.
This means that it is difcult to select the K value appropriately.
As shown in Fig.
2.39, when the K value is 3, the prediction result at the question mark will be a triangle and when the K value is 5, the prediction result at the question mark will become a square.
Figure 2.40 shows the decision boundary for different K values.
It can be found that as the value of K increases, the decision boundary will become smoother.
Generally speaking, a larger K value will reduce the impact of noise on classication but will make the boundaries of classes less obvious.
The larger the K value is, the more likely it is to cause under-tting, because that the decision boundaries are too blur.
Correspondingly, the smaller the K value is, the easier it is to cause over-tting, because that the decision boundaries are too sharp.
The KNN algorithm can not only be used for the classication problems, but also for the regression problems.
In the classication prediction problem, it normally adopts the majority voting method.
In the regression prediction problem, the average method is widely used.
Although these methods are seemingly only about the K samples of the nearest neighbors, the computation volume of the KNN algorithm is very large in fact.
This is because that the KNN algorithm needs to traverse all samples to determine which K samples are the nearest neighbors to the sample to be tested.
Naive Bayes is a simple multi-classication algorithm based on Bayes theorem and assumes that the features are independent.
Given the sample feature X, the proba- bility that the sample belongs to class c is P C c Xj P X C c j P C c P X Where, P(C cX) is called the posterior probability, P(C c) represents the prior probability of the target, and P(X) represents the prior probability of the feature.
Normally we do not consider P(X), because P(X) can be seen as a xed value when classifying, that is P C c Xj / P X C c j P C c P(C c) has nothing to do with X and needs to be determined before training the model.
Generally, the proportion of samples with category c in the dataset is calculated as P(C c). It can be seen that the core of classication is to nd P(X C c). Suppose feature X is composed of the following elements: X X, X, , X Generally, it can be easily calculated that Y P X C c j Combining the attribute conditional independence assumption, we can prove P X C c j Y P X C c j The attribute conditional independence assumption states that given the sample classication as a condition, the distribution of each attribute value is independent of the distribution of other attribute values.
The reason why Naive Bayes is naive is precisely because of the attribute independence assumption used in its model.
Making this assumption effectively simplies the calculation and gives the Bayesian classier a higher accuracy and training speed on large databases.
Here is an example.
We want to judge a persons gender C by his height X1 and weight X2.
Suppose that the probability of a person with a height of 180 cm and a height of 150 cm is male is 80% and 20%, respectively, and the probability of a person with a weight of 80 kg and 50 kg is male is 70% and 30%, respectively.
According to the Naive Bayes model, the probability that a person with a height of 180 cm and a weight of 50 kg is male is 0.8 0.3 0.24, while the probability that a person with a height of 150 cm and a weight of 80 kg is male is only 0.7 0.2 0.14.
It can be considered that the two features of height and weight independently contribute to the probability that this person is male.
The performance of the Naive Bayes model usually depends on the degree to which the feature independence hypothesis is satised.
As mentioned in the previous example, the two features of height and weight are not completely independent.
This correlation will inevitably affect the accuracy of the model, but as long as the correlation is not large, we can continue to use the Naive Bayes model.
In practical applications, different features are rarely completely independent.
Integrated learning is a machine learning paradigm.
In this paradigm, multiple learners are trained and combined to solve the same problem, as shown in Fig.
Thanks to the multiple learners involved, the generalization ability of ensemble learning can be much stronger than using a single learner.
Lets imagine you randomly ask a complicated question to several thousands of people, and then combine their answers together.
In most cases, this integrated answer is even better than an answer provided by an expert.
This is the collective intelligence we talk about.
The implementation methods of ensemble learning can be classied into two typesbagging and boosting.
Bagging independently builds several basic learners, and then averages their predictions.
Typical models of Bagging include random forests and so on.
On average, the prediction result of the combined learner is usually better than any single elementary learner because its variance is reduced.
Boosting constructs the basic learner in a sequential manner, and gradually reduces the deviation of the comprehensive learners prediction.
Typical models of Boosting include Adaboost, GBDT and XGboost.
In general, Bagging can reduce the vari- ance, thereby suppressing over-tting while Boosting focuses on reducing the deviation, thereby increasing the capacity of the model, but it may cause over-tting.
Random forest algorithm is a combination of the bagging method and the CART decision tree.
The overall process of the algorithm is shown in Fig.
Random forest algorithm can be used for classication and regression problems.
The basic principle is to build multiple decision trees and merge them to make more accurate and stable predictions.
During the training process of the decision tree, sampling is performed at the two levels of sample and feature at the same time.
At the sample level, the bootstrap sampling (sampling with replacement) is used to determine the sample subset used for decision tree training.
At the feature level, before each node of the decision tree is split, some features are randomly selected to calculate the information gain.
By synthesizing the prediction results of multiple decision trees, the random forest model can reduce the variance of a single decision tree model, but the effect of correcting the deviation is not satisfactory.
Therefore, the random forest model requires that every decision tree cannot be undertted, even if this require- ment may cause some decision trees to overt.
Also, note that each decision tree model in the random forest is independent, so the training and prediction processes can be performed in parallel.
Gradient boosting decision tree (GBDT) is one of the Boosting methods.
The predicted value of the model is the sum of the results of all decision trees.
The essence of GBDT is to continuously use new decision trees to learn the residuals of all previous decision trees, that is, the error between the predicted value and the true value.
As shown in Fig.
2.43, for a given sample, the prediction result of the rst decision tree is 20 years old, while the true age of the sample is 30.
The difference between the predicted result and the true value is 10.
If we can predict this difference with another decision tree, we can improve the prediction result of 20 and make it closer to 30.
Based on this idea, we introduce the second decision tree to learn the error of the rst decision tree, and so on.
Finally, the prediction results of the three learners are added together to get the true value of 30.
GBDT improves accuracy by continuously correcting the deviation of the decision tree, thus allowing a certain degree of undertting of the decision tree.
However, GBDT cannot correct the variance, so it is generally not allowed to overt the decision tree.
This is also one of the biggest differences between the boosting and bagging methods.
In addition, the training data of each decision tree in GBDT depends on the output of the previous decision tree, so the training process cannot be parallelized.
K-means clustering algorithm (K-Means clustering) is an algorithm that inputs the number of clusters K and a dataset containing n data objects, and outputs K clusters that meet the minimum variance standard, as shown in Fig.
It shows that the nal obtained cluster meets: the similarity of objects in the same cluster is higher and the similarity of objects in different clusters is lower.
Compared to the K-Means algorithm, the hierarchical clustering algorithm also outputs the tree-like relationship between the samples while outputting the clusters.
As shown in Fig.
2.45, the hierarchical clustering algorithm tries to divide the dataset at different levels so as to form a tree-shaped clustering structure.
The dataset can be divided either by a bottom-up agglomerative strategy, or a top-down divisive strategy.
The hierarchy of clusters is represented as a tree diagram, where the root of the tree represents the ancestor class of all samples, and the leaves are clusters with only one sample.
By the end of this chapter, we are about to review the overall process of a machine learning project with one case.
Suppose there is a dataset that gives the living area (1 square foot 0.09 square meter) and price of 21,613 houses sold in a certain city, as shown in Fig.
Based on such data, we hope to train a model to predict the prices of other houses in the city.
It can be inferred from the data in the house price dataset that the input (house area) and output (price) in the data are continuous values, so the regression model in supervised learning can be used.
The goal of the project is to build a model function h(x) to make the model innitely approximate the function that expresses the true distribution of the dataset.
Figure 2.47 shows a scatter plot of the data and a possible model function.
The goal of linear regression is to nd a straight line that best ts the dataset, that is, to determine the parameters in the model.
In order to nd the best parameters, we need to construct a loss function and nd the parameter value when the loss function reaches the minimum value.
The equation of the loss function is as follows: X J w 2m h x y Where m represents the number of samples, h(x) is the predicted value, and y is the true value.
Intuitively, the loss function represents the sum of squared errors from all samples to the model function, as shown in Fig.
When this loss function is reduced to the minimum, all samples should be evenly distributed on both sides of the tted straight line.
At this time, the tted straight line is the model function we require.
As mentioned earlier, the gradient descent algorithm uses an iterative method to nd the minimum value of a function.
The gradient descent algorithm rst randomly selects an initial point on the loss function, and then nds the global minimum value of the loss function according to the negative gradient direction.
The parameter value at this time is the best parameter value we require, as shown in Fig.
Point A represents the position where the parameter w is randomly initialized point B represents the global minimum of the loss function, which is the nal parameter value the connected line between A and B represents the trajectory formed by the negative gradient direction.
For every iteration, the value of parameter w will change, resulting in the constant change of the regression line.
Figure 2.50 shows an example of an iterative process using gradient descent.
It can be observed that as the points on the loss surface gradually approach the lowest point, the linear regression tting line ts the data better and better.
Finally, we can get the best model function h(x) 280.62x 43,581.
After the model training is completed, we need to use the test set for testing to ensure that the model has sufcient generalization capabilities.
If there is overtting in the test, we can add a regular term to the loss function and adjust the hyperparameters.
If it is under-tting, we can use more complex regression models, such as GBDT.
After that, the model needs to be retrained, and the test set is reused for testing until the generalization ability of the model meets expectations.
It should be noted that since real data is used in the project, the role of data cleaning and feature selection cannot be ignored either.
This chapter mainly introduces the denition, classication and major challenges of machine learning.
Meanwhile, the overall process of machine learning (data collec- training, model tion, data cleaning, feature extraction and selection, model evaluation and testing, model deployment and integration, etc.), common machine learning algorithms (linear regression, logistic regression, decision trees, support vector machines, naive Bayes, KNN, ensemble learning, K-Means, etc.), gradient descent algorithms, hyperparameters and other important machine learning knowl- edge are sorted out and explained nally, through the use of linear regression, the housing price prediction case is completed, showcasing the overall process of machine learning.
Machine learning is the core technology of articial intelligence.
Please tell us the denition of machine learning.
The generalization error of the model can be classied into variance, bias, and irresolvable errors.
What is the difference between variance and bias? What are the characteristics of the variance and bias of an overtting model? 3.
In accordance with the confusion matrix shown in Fig.
2.25, please nd the F1 value.
In machine learning, the entire dataset is generally divided into three parts: training set, validation set, and test set.
What is the difference between the verication set and the test set? Why introduce a validation set? 5.
Linear regression models use linear functions to t the data.
For nonlinear data, how to deal with the linear regression model? 6.
Many classication models can only handle two classication problems.
Taking SVM as an example, try to nd a solution for multi-classication problems.
Please refer to the relevant information and answer how does the Gaussian kernel function in SVM map features to innite dimensional space? 8.
Is gradient descent the only way to train the model? What are the limitations of this method? As a machine learning model based on neural networks, deep learning is particularly advantageous in elds like computer vision, speech recognition and natural language processing.
This chapter mainly introduces the basic knowledge related to deep learning, including the history, the components of neural networks, the types of deep learning neural networks, and the common problems researchers may encounter in the deep learning projects.
In traditional machine learning, the features are selected by hands.
The more features are included, the more information can the model transmit to the outside world, and thus the model is deemed as more expressive.
However, as the number of features increases, the complexity of the algorithm also enhances, and the searching space of the model will also expand accordingly.
The training data will look sparse in the feature space and affect the judgment of similarity.
This is the so-called explosion of dimensionality.
What is more important, if the features are not conducive to the task, they may interfere with the learning efciency.
Limited by the amount of features, the traditional machine learning algorithms are more suitable for the training of small data.
When the scale of data accumulates to a certain degree, it will be difcult to ameliorate the performance simply by adding more data.
Therefore, traditional machine learning has low requirements for computer hardware and limited needs of calculation, thus generally does not need GPUs and graphics cards to support the parallel operations.
Figure 3.1 shows the general process of traditional machine learning, where the features are highly interpretable as they are selected manually.
However, as it is not necessarily the more features the better, high-quality features will decide if the model can perform successful recognition.
How many features are required should be determined by what the problem of the model is designed to solve.
In order to avoid the bias that are internet to peoples selection of features, deep learning pursues algorithm that can automatically extract features.
Although this reduces the interpretability of features, it improves the adaptability of the model to different problems.
In addition, deep learning adopts an end-to-end learning mode and is combined with high-dimensional weight parameters, which makes it capable of achieving higher performance by handling large training data than the traditional ways.
The massive data also put more requirements on the hardware: the large number of matrix operations are processed slowly on the CPU, thus the GPU needs to be included to provide parallel acceleration.
Generally speaking, deep learning is based on deep neural networks, namely the multi-layer neural networks.
This is a model built to simulate the human neural network.
As shown in Fig.
3.2, a deep neural network is made of a group of stacked perceptrons, and a perceptron simulates the neurons in human brain.
In Fig.
3.2b, c, each circle represents a neuron.
In the following passages, we will see the similarities between this design and the neurons.
The design and application research of articial neural networks will normally need to take three things into consideration, namely neuron as a function, the connection between neurons, and learning (training) of network.
What exactly is a neural network? At present, people havent reached a consensus on the denition of neural network.
According to the American neural network scientist Robert Hecht-Nielsen, a neural network is a computing system made up of a number of very simple, highly interconnected processing elements which are formed in a certain way, which process information by their dynamic state response to external inputs.
Combining the origin, characteristics and various interpretations of the neural network, it can be simply put as: articial neural network is an information processing system that is designed to imitate the structure and functions of the human brain.
The articial neural networks feature the basic characteristics of human brain, such as parallel information processing, learning, association, classi- cation, and memorizing.
Or we can just say that the articial neural network is a network interconnected by a number of articial neurons, an abstract and simplied version of human brain in terms of the microstructure and function and is an important approach to imitate human intelligence.
In fact, the history of deep learning is the history of neural networks.
Since the last 1950s, with the ongoing development of computer hardware, neural networks developed from an initial single layer to multiple layers, and eventually became the deep neural network well-known today.
Generally speaking, the development of neural networks can be divided into three stages, as shown in Fig.
In 1958, Frank Rosenblatt invented the Perceptron algorithm, which marked the beginning of the initial development stage of neural networks.
However, as machine learning had not been an independent subject of articial intelligence research, the development of perceptron algorithm was restrained.
In 1969, Marvin Lee Minsky, a pioneer of articial intelligence from the US, questioned that perceptrons could only handle linear classication problems but not even the simplest XOR problem.
His questioning and the doubts followed were like a death penalty of the perceptron algorithm, and also heralded the coming winter of deep learning that lasted for two decades.
In 1986, Geoffrey Hinton invented the Multi-Layer Perceptron (MLP) and improved the situation.
Hinton proposed to use a sigmoid function to perform nonlinear mapping on the output of perceptron, which effectively solved the problem of nonlinear classication and learning.
In addition, Hinton also invented the Back Propagation (BP) algorithm to train MLP.
The algorithm and its derivative algo- rithms are still used for deep neural network training even today.
In 1989, Robert Hecht-Nielsen proved the universal approximation theorem (UAT). The theorem states that any continuous function f in a specic closed range can be approximated by a BP network with one hidden layer.
In short, a neural network can t any continuous function.
In 1995, Vladimir Vapnik and Corinna Cortes proposed sup- port vector machine (SVM), one of the most decisive breakthroughs in machine learning.
The algorithm not only solid in theoretical foundation, but also has outstanding performance in experiments.
In 1998, a number of neural networks were invented, including the well-recognized convolutional neural network (CNN) and recurrent neural network (RNN). However, due to the vanishing gradient and gradient explosion problems that occasionally occurred during the over-deep train- ing of neural network, the neural network was once gain gradually forgotten by the public.
The year 2006 marks the advent of the era of deep learning.
In this year, Hinton proposed the combination of unsupervised pre-training and supervised ne-tuning as a solution to the gradient vanishing problem of deep neural network training.
In 2012, the AlexNet designed by Hintons team won the worlds top image recogni- tion competition ImageNet by defeating all the other methods, which triggered a heat of deep learning.
In 2016, AlphaGo, a deep learning articial intelligence program of Google, defeated the world Go champion and nine-dan Go player Lee sedol.
The victory brought the worlds attention to deep learning to a new level.
The single-layer perceptron is the simplest type of neural network.
As shown in Fig.
3.4, the input vector X x, x, , x rstly calculates the dot product with the weight W w, w, , w, denoted as net.
Among them, x equals to 1, and w is called bias.
For regression, net can be directly used as the perceptron output.
And for classication problems, it needs to let net to be activated by an activation function called Sgn(net) so as to be taken an output.
The Sign function equals 1 in the range of x 0, and equals 1 if it is the other way around.
The perceptron shown in Fig.
3.4 is actually like a classier which adopts a high-dimensional input vector x to classify the input samples in dichotomies in the high-dimensional spaces.
To put it more specically, when Sgn(net) is 1, it means that the sample is classied as positive, and if Sgn(net) is 1, then the sample is classied as negative.
The boundary of the two situations is WX 0, a hyperplane in high-dimensional space.
In nature, the perceptron is nothing but a linear model, which means that it can only implement linear classication but cannot handle nonlinear data.
As shown in Fig.
3.5, for AND and OR logical operators, the perceptron can easily nd a line to classify them correctly.
But it can do nothing about the exclusive OR (XOR) logical operations, which is the example that Minsky quoted to prove the limitations of perceptron in a straight-forward manner.
In order to let the perceptron to process non-linear data, people produced multi- layer perceptron, also known as feedforward neural network, as shown in Fig.
Feedforward neural network is a version neural network with the simplest architec- ture, featuring hierarchically arranged neurons (perceptrons), and currently one of the most widely applied and fastest-growing articial neural networks.
In Fig.
3.6, the leftmost three neurons in the multilayer perceptron constitute the input layer of the entire network.
The neurons in the input layer do not apply any operations.
They simply represent the value of each input vector.
Except for the input layer, the neuron in each layer of nodes that possesses a computing function is called a computing unit.
The neurons in each layer receive the output values of the neurons in the previous layer as input and transmit them to the next.
The neurons in the same layer are not connected, and information can only be transmitted one-way between the different layers.
The XOR problem can be solved by merely a very simple multilayer perceptron.
In Fig.
3.7a, we can nd the structure of multilayer perceptron.
The solid line designates that the weight is 1, and the dashed line indicates the weight is 1.
The number in circle is the offset.
For example, for the point (0,1) x 0, x 1 The output of the neuron 1.5 is sgn x x 1:5 sgn 0:5 1 As the two lines on the left side of the 1.5 neuron are both solid, we can conclude the coefcients of x and x are both 1.
So the output of this neuron 0.5 is sgn x x 0:5 sgn 0:5 1 The coefcients of x and x are both 1, as the two lines on the left side of the 0.5 neuron are both dashed lines.
The output of the rightmost neuron is sgn 1 1 1 sgn 1 1 The two 1 on the left side of the equation are the outputs of the neuron 1.5 and the neuron 0.5, and +1 is the offset of the neuron output.
Our readers can try to verify, on your own, that for the points (0, 0), (1, 0) and (1, 1), the output of the multilayer perceptron is 1, 1, and 1 respectively, consistent with the results of the XOR operation.
In fact, the two neurons 1.5 and 0.5 are respectively represented by the upper right and lower left lines shown in Fig.
3.7b, adopting the linear classier to implement the classication of nonlinear samples.
With the increase of hidden layers, the nonlinear classication of neural networks is gradually enhanced, as shown in Fig.
The core of training a machine learning model is loss function, and for deep learning, it is also the same.
This section will introduce the rules of model training in deep learning by elaborating on the loss function, including the gradient descent and backpropagation algorithms.
While training a deep neural network, it rstly needs to create a function to detect the error of the target classication.
And we call it a loss function (or error function). The loss function reects the error between the target output value and the actual output value of the perceptron.
The most commonly adopted loss function is the mean square error (MSE), with the formula as follows: J w 1 2n X t o In this formula, w the model parameter, X training examples set, n the size of X, D the gathering of neurons in the output layer, t the target output, and o the actual output.
Although the parameter w is not directly included in the right side of the equation, the actual output o will need to be calculated in the model, therefore it is decided by the value of the parameter w.
Once the training example is settled, both t and o are constants.
The actual output of the loss function changes with the value of w, so w is the independent variable of the error function.
The feature of the loss function MSE is that its major body is the sum of squares of error, while the error is the difference between the target output t and the actual output o.
In the formular, another more complicated component is the 1/2 coefcient.
In the following pas- sages, we will learn that this coefcient can bring a more concise form for loss function derivation, by offsetting the exponent 2 into one.
Cross-entropy error is another common loss function, and its formula is as follows: J w 1 n X t ln o 1 t ln 1 o The symbols in the formula suggest the same with those in the MSE formula.
Cross-entropy error depicts the distance between the two probability distributions.
Generally speaking, the MSE function mainly deals with regression, while the cross- entropy error function is more used on classication.
The target of model training is to search for the weight vector that minimizes the loss function.
However, the neural network model is very complicated, and there is no effective mathematical method to nd analytical solutions.
Therefore, we need to use the gradient descent to nd a numerical solution to minimize loss function.
The gradient of the multivariate function f(x, x, . . ., x) at x is f x, x, , x f x , f x , , f x xj The gradient vector indicates the direction of most rapid increase of the function.
The negative gradient vector, correspondently, indicates the direction of the steepest decrease of the function.
The gradient descent algorithm is used to let the loss function search along the direction of the negative gradient, iteratively update the parameters, and minimize the loss function.
Each sample in the training examples set X is x, t, where x is the input vector, t is the target output, o is the actual output, and is the learning rate.
Figure 3.9 shows the pseudo-code for the batch gradient descent (BGD) algorithm.
As a derivative of the direct application of gradient descent to deep learning, the BGD algorithm is actually not used that frequently.
The major deciency of this algorithm is that every time the weight is updated, it needs to compute all training examples, slowing down the convergence.
To deal with this problem, a common variant of the gradient descent, the stochastic gradient descent (SGD) algorithm is invented, which is also known as the incremental gradient descent algorithm.
Figure 3.10 shows the pseudo-code for SGD algorithm.
SGD algorithm selects one sample at a time to update the gradient.
One of the advantages of this method is that the dataset can be expanded during the model training.
And this mode of training model during data collection is known as online learning.
Compared with the batch gradient descent algorithm, the stochastic gradi- ent descent algorithm enhances the frequency of weight updating but goes from one extreme to the other.
Normally, noise exists in training samples.
Batch gradient descent can reduce the inuence of noise by taking the average of the gradients of the samples.
However, as stochastic gradient descent analyzes one sample at a time during weight updating, when it comes to the phase of accurately approximating the extreme value, the gradient may bounce around it, and unable to converge to the extreme value.
The gradient descent algorithm most commonly used in practical tasks is the mini-batch gradient descent (MBGD) algorithm, as shown in Fig.
To tackle deciencies of the above two gradient descent algorithms, the mini-batch gradient descent algorithm uses a small batch of samples at a time when updating the weight, giving consideration to both the efciency and the gradients stability.
The size of a minibatch is generally of 128 elements, but also varies depending on the different situations.
Applying the gradient descent will need to calculate the gradient of the loss function.
For the traditional machine learning algorithms, such as linear regression and support vector machines, computing the gradient manually is achievable.
However, the model function of the neural network is much more complicated, and it is impossible to nd gradients of the loss function with respect to all the parameters this backdrop, Geoffrey Hinton invented the by just one formula.
Against backpropagation algorithm, which can update the weights by layers separately through the backpropagation, and effectively speed up the neural network training.
The back propagation of errors works in the opposite direction of the forward propagation, as shown in Fig.
J w 1 2n X t o For each training example x, t in the training examples set X, the output of the model is denoted as o.
Assuming that the loss function takes the mean squared error: J w 1 2n X t o Suppose that there are L layers in the model (not including the input layer), and the parameters of the rst layer are noted as w.
We can observe that the value of J(w) is not the minimum during iterations because of the deviation between w and optimal parameter values, and this observation works for every layer.
In other words, the value of a loss function value is caused by the error of parameters.
During the forward propagation process, each layer will cause a certain error.
As these errors accumulating layer by layer, they are eventually manifested in the form of a loss function in the output layer.
When there is no given model function, we cannot be sure about the relationship between a loss function and the parameters.
However, the relationship between the loss function and the model output J/o is evident.
This is the key to understand the backpropagation algorithm.
Suppose the output of the penultimate layer as o, and the activation function of the output layer as f, then the loss function can be extended to: J w 1 2m X t f wo In this formula, o is only relevant to w, w, , w . It can be observed that the loss function can be divided into two parts, the section caused by w and the section caused by other parameters.
The latter acts on the loss function as output in the penultimate layer through accumulating errors.
Based on the J/o obtained above, we can relatively easily calculate J/o and J/w.
In this way, we will get the gradient of the loss function with respect to the output layer parameters.
Obviously, is involved in the computing of J/o the derivative of activation function f wo and J/w as weight.
When the derivative of the activation function is eternally less than 1 (as how Sigmoid function works), the value of J/o will keep decreas- ing during the backpropagation.
This phenomenon is known as the vanishing gradient problem, which will be introduced more specically in the following passages.
As for the parameters in other layers, we can infer similarly from the relationship between J/o and J/o . Briey speaking, the backpropagation is the process of distributing errors layer by layer and is essentially an algorithm that applies the chain rule to calculate the loss function in respect to the parameters of each layer.
In general, the backpropagation algorithm works as shown by Fig.
In the code above, represents multiplying by element, and f is the activation function.
It is worth noticing that the output of the no.
i layer is also the input of the no.
i + 1 layer.
The output of layer no.
0 is deemed as the input of the entire network.
Additionally, when the activation function is Sigmoid function, it can be proved that: J f x f x 1 f x Therefore, f (ol 1) in the algorithm can also be written as ol(1 ol).
Activation function plays a very important role in learning neural network models and interpreting complex nonlinear functions.
The activation function adds nonlinear characteristics to the neural network.
Without the activation function, a neural network can only represent one linear function, no matter how many layers it has.
However, the complexity of linear function is limited, and it is even less capable to learn mapping of complex function from data.
This section introduces the activation functions commonly used in deep learning and elaborates on their advan- tages and disadvantages.
Our readers can choose from the list based on their demands while doing projects.
As shown in Fig.
3.14a, Sigmoid function is the activation function most fre- quently adopted in the early stage of the study of feedforward neural network.
Similar to the logistic regression model, the sigmoid function can be used in the output layer for binary classication.
In general, a sigmoid function is monotonic and continuous, with the derivative easy to compute and output bounded.
These features facilitate the convergence of network.
However, we can see the derivative of sigmoid function approaches 0 when far away from the origin.
When the network is very deep, the backpropagation will drive an increasing number of neurons into the saturation regions, reducing the scope of gradients.
Generally speaking, a sigmoid network will degenerate to 0 within 5 layers, which disabling the neural network from further training.
This phenomenon is the so-called vanishing gradient problem.
Another disadvantage of sigmoid function is that its output is not zero-centered.
As shown in Fig.
3.14b, tanh function is a main alternative to Sigmoid.
The tanh activation function recties the disadvantage of being not zero-centered in outputs and the gradient is more like the natural gradient in gradient descent, thus reducing the number of iterations required.
However, the tanh function is still prone to saturation just like the sigmoid function.
As shown in Fig.
3.14c, Softsign function reduces the saturability of the tanh function and sigmoid function.
But be it Softsign, tanh or Sigmoid, an activation function is more likely to cause vanishing gradient problem.
At a position far away enough from the central point (the origin) of the function, the derivative of activation function will always approach 0, stopping the update of weights.
As shown in Fig.
3.14d, the rectied linear function or ReLU is currently the most widely applied activation function.
Unlike the activation functions such as Sigmoid, ReLU is not bounded by upper limit, so neurons will never reach saturation, which effectively alleviates the vanishing gradient and can converge much faster in gradi- ent descent.
Experiments show that the neural network using the ReLU activation function can also generate good performance even without unsupervised pre-training.
In addition, the functions like Sigmoid all require exponential equa- tions, which is quite computing-intensive.
However, the ReLU activation function spares much efforts in calculation and computation.
Although ReLU has lots of advantages, it also has disadvantages.
Since the ReLU function has no upper limit, it may diverge during training.
Secondly, the ReLU function is not directable at zero, thus it will not be smooth enough in some regression problems.
What is mor, the value of ReLU function will remain constantly zero when inputs are negative, which may cause the death of neurons.
As shown in Fig.
3.14e, Softplus function is modied on the basis of ReLU.
Although the softplus function require more computation than ReLU, it has a continuous derivative, and the curve is relatively smoother.
Softmax function is an extension of the sigmoid function in multiple-dimensional cases.
Its function expression is as follows: z e P e The softmax function is designed to map an arbitrary vector of real numbers of one K dimension to the probability distribution of another K dimension.
Therefore, the softmax function is often used as the output layer for multi-classication.
Regularization is a rather important and highly effective measure to reduce gener- alization error in machine learning.
Compared to the traditional machine learning model, the capacity of deep learning model is larger, so it is more likely to lead to overtting.
To this end, the researchers have worked out several useful techniques to prevent overtting, including: 1.
Parameter norm penalty, which adds norm constraints such as L1 and L2.
Dataset expansion, such as adding noise and changing data.
Dropout.
Early stopping.
This section will introduce these techniques successively.
For many regularization methods, they constrain the learning capability of the model by adding a parameter norm penalty Z(w) to the objective function J: eJ J aZ w Where a is a non-negative penalty term coefcient whose value weights the relative contribution of the norm penalty term Z and the standard objective function J to the general objective function.
Setting a to zero results in no regularization and the larger values of a, the greater regularization will be correspondingly.
So a is a hyperparameter.
It is worth noticing that in deep learning we normally only penalize the model parameter w, rather than the biases.
This is because that, generally speaking, the biases require less data to accurately t, while adding penalties to the bias parameters will often cause undertting.
Different Z will generate different methods of regularization.
This section will mainly introduce two of them: L1 regularization and L2 regularization.
Among the linear regression models, L1 regularization can lead to Lasso regression, and L2 regularization can lead to the ridge regression.
In fact, the so-called L1 and L2 denote norms.
The L1 norm of a vector is dened as X j j wk k w That is, the sum of the absolute values of all components of the vector.
It can be proved that the gradient of L1 norm is Sgn(w). Consequently, the gradient descent algorithm can be used to calculate the L1 regularization model.
The L2 norm is the commonly known Euclidean distance: wk k s X w Since the L2 norm has a quite wide application, thus it is often abbreviated as w with the subscript being omitted.
However, since the gradient of the L2 norm is relatively complicated, the L2 regularization is generally noted as: Z w 1 wk k We can see that the penalty term of L2 regularization is proved to be w after the derivative.
Therefore, when performing gradient descent on the L2 regularization model, the equation to update weights should be rectied as w 1 a w J Compared to the common gradient update equation, it is equivalent to the parameter multiplying a reduction factor, so as to restrain the growth of parameter.
Figure 3.15 displays the difference between L1 regularization and L2 regulariza- tion.
The contour lines represent the standard objective function J, and the diamond or circle centered at the origin represents the regularizer.
The geometric signicance of the parameter norm penalty is that for any point in the feature space, what should be considered is not only the standard objective function value corresponding to that point, but also the size of the geometric shapes corresponding to the regularizer of the point.
It is not difcult to infer that the larger the penalty coefcient a is, the more likely the diamond or circle will turn smaller, and the closer the parameter will incline to the origin.
As can be seen from Fig.
3.15 that the parameter that can stabilize the L1 regularization model is highly probable to appear at the corners of the diamond shape, which suggests that the parameters of the L1 regularization model are likely to be sparse matrices.
The example in the gure reveals that the optimal parameter in correspondence to w1 is denoted as zero.
Therefore, the L1 regularization can work as a method of feature selection.
To analyze it from the perspective of probability distribution, for many norm constraints, it is simply adding prior distributions to the parameters.
The L2 norm is equivalent to parameters obeying the Gaussian prior distribution, and the L1 norm is parameters obeying the Laplace prior distribution.
The most effective method to prevent overtting is increasing the size of training set, as the probability of overtting will decrease with the increase of training set.
But collecting data (especially the labeling data) is a time-consuming and costly task.
To this end, dataset expansion is an alternative that is less time-consuming but as efcient, although there is hardly a universal method of expansion for the different datasets in different tasks.
In target recognition, the popular methods of dataset expansion include image rotation, image scaling and so on.
The premise of transforming the forms of images is that the category of the image is changed.
For instance, while recognizing handwritten numbers, 6 and 9 are two categories that can be easily confused after rotation thus need special attention.
In speech recognition, it often adds random noise into the input data.
And in natural language recognition, a common method is replacing the synonyms.
Noise injection is a popular approach of dataset expansion.
The noise can be injected to either the input layer, the hidden layer or the output layer.
For Softmax classication, the noise I added to the output layer through Label Smoothing.
Suppose that there are K candidate classes for a classication task, the standard output provided by the dataset is generally represented as a one-hot encoded K-dimensional vector.
The element that represents the correct class is 1, and 0 for the rest.
By injecting noise, the element corresponding to the correct category will turn to 1 (k 1)e/k, and the rest will be e/k, where e represents a constant that is sufciently small.
Intuitively, label smoothing reduces the difference between the correct sample and the wrong sample in terms of label value, which increases the difculty of model training.
For an overtting model, it will effectively reduce the overtting problem, thereby facilitating the model performance.
Dropout is a regularization method that is very popular and simple in computation, widely applied since it was proposed in 2014.
To put it simply, Dropout is to randomly discard a part of the output of neurons during the training phase.
The parameters of these discarded neurons will not be updated.
By randomly dropping the output, Dropout builds a series of subnets featuring different structures, as shown in Fig.
These subnets will converge in a certain way in the same deep neural network, which equals to taking the method of integrated learning.
While running the model, we will leverage the collective intelligence of all the trained subnets, so that the neuron outputs will no longer be discarded.
Dropout is less complex in computation and easier in implementation compared to parameter norm penalty.
The random process of Dropout during the training is neither a sufcient condition nor a necessary condition, thus it can absolutely make constant shields parameters, and generate competitive models.
Early stopping of training should be allowed.
As shown in Fig.
3.17, examining the validation dataset regularly, and when the loss function of the validation dataset starts to rise, the training can be stopped in advance to avoid overtting.
However, it may also bring the risk of undertting.
This is because that the samples in validation dataset are not sufcient enough, which makes it difcult to stop training at the exact moment when the generalization error of the model is the minimum.
In some extreme cases, the generalization error of the model on the validation dataset may rapidly drop soon after a slight uplift while an early stopping of the training will cause the model undertting.
There are various optimized versions of the gradient descent algorithm.
In the implementation of object-oriented language, it often encapsulates different gradient descent algorithms into one object, which is called an optimizer.
The optimizers we commonly use include SGD, Momentum, Nesterov, Adagrad, Adadelta, RMSprop, Adam, Adamax and Nadam, etc.
These optimizers mainly improve algorithm in terms of the convergence speed, the stability after converging to a local extremum, and the efciency of adjusting hyperparameters.
This section will introduce the most commonly used optimizers and their design.
The Momentum optimizer is a fundamental improvement of the gradient descent algorithm, which supplements momentum term to the weight update equation, as shown in Fig.
Suppose the value of weight change in the n-th iteration is d(n), then the weight update equation shall turn into: d n J ad n 1 Where a is a constant number between 0 and 1, known as momentum, and ad(n 1) is called the momentum term.
Lets imagine a small ball rolling down along the error surface from a random point.
The ordinary gradient descent algorithm will drive the ball moving along the force curve, but it is actually against the laws of physics.
The real situation is that the momentum of the small ball will accumulate as it is rolling down, and its speed will increase faster along the downhill direction.
In places where the gradient is relatively stable, the small ball will roll much faster so as to quickly pass the at region.
In consequence, the model convergence will accelerate.
On the other hand, as shown in Fig.
3.19, the momentum term recties the direction of the gradient and reduces the occurrence of abrupt changes.
In addition, a small ball carried with inertia is more likely to overcome the narrow local extrema, thus the model will be less likely to get stuck in the local extreme points.
The disadvantage of the momentum optimizer is that the momentum may carry the small ball so far away that it may miss the optimal solution, thus additional iterations will be needed for convergence.
Secondly, the learning rate and momen- tum a of the momentum optimizer are set manually.
It means that a lot of experi- ments should be carried out to nd the values that are appropriate to use.
A shared characteristic of stochastic gradient descent, mini-batch gradient descent and momentum optimizer is that each parameter is updated with one same learning rate.
However, for Adagrad optimizer, different parameters should adopt different learning rates.
The gradient update equation of the Adagrad optimizer is generally written as w p P g i e g n In the equation, g(n) represents the gradient dJ/dw of the cost function in the n-th iteration, and e is a small constant.
With the increase of n, the denominator of the equation will increase, and the degree of the weight update will gradually decrease, which is equivalent to learning rate having been reduced dynamically.
In the initial stage of model training, the initial value is not even close to the optimal solution of the loss function, so it requires a much higher learning rate.
But as the frequency of updates increases, the weight parameters will approximate closer to the optimal solution, and the learning rate will keep decreasing accordingly.
The advantage of the Adagrad optimizer is that it can automatically update the learning rate, but this feature also brings some disadvantages.
Since the update of the learning rate is decided by the gradient of the past iterations, it is likely that the learning rate could already have been reduced to zero when the weight parameters are still far from the optimal solution.
If so, the optimization process will become meaningless.
The Root Mean Square Propagation (RMSprop) optimizer is an ameliorated version of the Adagrad optimizer by introducing an attenuation coefcient, to the algorithm, which attenuates the historical records gradient by a certain percentage for every iteration.
The gradient update equations of the RMSprop optimizer is as follows: r n br n 1 1 b g n w p g n r n e In the equations, b refers to the attenuation factor and e is a small constant.
Because of the effect of attenuation factor, r does not have to increase monotonically along with the increase of n.
This can effectively prevent the Adagrad optimizer from stopping learning too early, and is very suitable for handling non-stationary targets, especially conducive to the recurrent neural networks.
The Adaptive Moment Estimation (Adam) optimizer is currently the most widely used optimizer that evolved from Adagrad and Adadelta.
Adam aims at identifying an adaptive learning rate for each parameter, which is useful in a complex network structure, as the sensitivity different parts of the network to weight adjustment is different, and the learning rate for the sensitive parts should generally be lower.
It is difcult and complicated for people to identify the sensitive parts and calculate the specic learning rate manually.
The gradient update equation of the Adam optimizer is similar to that of the RMSprop optimizer, as shown below: w p m n v n e Where m and v represent the estimates of the rst moment (mean) and second moment (uncentered variance) of the past gradients respectively.
Resembling the attenuation-based formula proposed by the RMSprop optimizer, m and v can be dened as: m n am n 1 1 a g n v n bv n 1 1 b g n In form, m and v are the moving averages of gradient and squares of the gradient respectively.
But dening as such will make the algorithm quit unstable in the rst several iterations.
Lets assume that both m(0) and v(0) are valued zero, so when a and b approximate one, m and v will be very close to zero in the initial iteration.
In order to counteract this problem, the nal equations put in use will be as: em n m n 1 a ev n v n 1 b Although the learning rate, a, and b in the Adam optimizer need to be set up manually, the difculty has been largely reduced in terms of implementation.
The experiments have proved that the default value of a and b is 0.9 and 0.999 respectively, and that of the learning rate is 0.0001.
In the practice, the Adam optimizer will facilitate rapid convergence.
When the algorithm converges to the point of saturation, the learning rate can be lowered accordingly, and other param- eters can stay unadjusted.
With several times of the reduction of learning rate, the model will converge to a proper and satisfactory extreme value.
Since the earliest BP neural network, people have designed a wide range of neural In the domain of computer vision, networks to handle different problems.
convolutional neural network is the most widely used deep model currently.
In the domain of natural language processing, recurrent neural network once outshined the other models.
This section will also introduce a generative model based on game theorygenerative adversarial network.
Convolutional neural networks (CNNs) are a kind of feedforward neural network.
Unlike the fully connected neural networks (FCNNs), the articial neurons of the convolutional neural networks can respond to a part of the units within the range of coverage, and have good performance in image processing.
Generally speaking, a convolutional neural network consists of convolutional layers, pooling layers, and fully connected layers.
In the 1960s, in their study of the neurons of cats cortex that are sensitive to locality and used to select direction, David Hubel and Torsten Wiesel discovered that the unique network structure of cats neurons can effectively reduce the complexity of the feedback neural network.
Inspired by their discovery, convolutional neural networks were proposed.
Today, convolutional neural networks have become one of the heatedly discussed subjects in many scientic elds, particularly in pattern recognition.
As the structure of CNNs can avoid the complicated image preprocessing and can directly input the original image, it has been extensively applied in many elds.
The name convolutional neural network derives from a mathematical operation called convolution.
Convolution is the operation of performing inner product to image (or feature map) and lter matrix (also called lter and convolution kernel).
The image is the input of the neural network, and the feature map is the output of convolutional layer or pooling layer of the neural network.
Their difference is that the value in the feature map is the output of the neuron thus it does not have limitation theoretically, but the value of the image is in line with the brightness of the three channels RGB, valued from 0 to 255.
Each convolutional layer in the neural network corresponds to one or more lter matrices, and it works differently from a fully connected neural network.
Every neuron in the convolutional layer can only take the output of neurons in a certain local window of the previous layer as input instead of being able to receive the output of all of neurons of the previous layer simultaneously.
This feature of convolution is known as local perception.
Generally speaking, peoples perceptions on the outside world evolves from the local features to the global features.
For the spatial relationship of an image, it is also the same that the local pixels are more closely related, while the distant pixels are less related.
Therefore, actually, there is no need for each neuron to perceive the global image.
It only needs to perform local perception, and synthesizes the data perceived locally at an upper level to get the global information.
The idea of partially connected network is inspired by the structure of the biological visual system where the neurons in the visual cortex also only respond to the stimuli in a certain region and receive information locally.
Another feature of convolution is the parameter-sharing.
For the input image, it can be scanned by one or more convolution kernels.
The parameters in the convo- lution kernel equal to the models weights.
In a convolutional layer, all neurons share the same convolution kernel and therefore also share the same weights.
Weight- sharing means that every time when a convolution kernel is traversing the image, its parameters will not change.
For example, a convolutional layer has three feature convolution kernels, and every convolution kernel will scan the entire image.
During the scanning, the parameter values of the convolution kernel are xed, and namely the pixels of the entire image share the same weights.
It means that the features learned in a certain part of the image can also be applied to other parts or other images.
This feature is known as position invariance.
Convolutional Layer image is the input.
The input Figure 3.20 displays a typical architecture of convolutional neural network.
image rst passes through a The leftmost convolutional layer consisting of three convolution kernels to acquire three feature maps.
The parameters of these three convolution kernels are independent from each other, and can be calculated and optimized by the back-propagation algorithm.
While performing the convolution operations, a window of the input image is projected to a neuron on the feature map.
The convolution operation is performed to detect different features of the input.
The rst convolutional layer may only present some low-level features, such as the edges, lines and corners of the image.
And the following layers can learn more complex features from the low-level features through iterations.
Looking at the convolution operation shown in Fig.
You can divide up to 3 3 different regions in a 5 5 square matrix, and the shape of these regions is identical to the convolution kernel.
Therefore, the dimension of the feature map is 3 3.
As shown in Fig.
3.22, each element in the feature map is calculated by multiplying one section of the original image and the convolution kernel.
In the matrix on the left of Fig.
3.22, the 3 3 region in the upper left corner is the part relevant to the upper left element in the feature map.
Multiplying each element of this part and the corresponding element of the convolution kernel, and summing the result, we will get the rst element 4 of the feature map.
The example shown here does not include the bias term, namely, the bias equals to zero.
In the more common convolution operations, it is often required to sum the result and the bias term after the multiplications (dot product), so as to output as a feature map.
The role of bias term here is similar to that in linear regression.
A convolutional layer is majorly structured by multi-channel convolution.
As shown in Fig.
3.23, a convolutional layer can house multiple convolution kernels and biases.
A combination of convolution kernel and bias is able to project the input tensor onto a feature map.
The role of multi-channel convolution is to collage all the feature maps generated by the convolution kernels and the bias terms to form a three-dimensional matrix as the output.
Generally speaking, the input and output tensor and the convolution kernel are three-dimensional matri- ces, featuring three dimensions referring to width, height and depth respectively.
In order to extend the above-mentioned convolution to the three-dimensional sphere, it need to make the rule that the depth of each convolution kernel should be the same with the input tensor.
This ensures that the depth of the feature map is one.
The convolution corresponding to each single convolution kernel operation does not have specic requirement on the width and height of the convolution kernel, but they are generally valued the same in the operation for the sake of convenience.
In addition, in order to stack the feature maps generated by different convolution kernels together, the feature maps need to have the same width and height.
In other words, all the convolution kernels in a same convolu- tion layer need to be of the same size.
As the output of the convolutional layer, the feature maps need to be activated.
Sometimes, the activation function can be taken as a consisting part of the convolutional layer.
But as the activation function and the convolution operation are not so relevant, we can also take activation function as an independent layer.
The most commonly used activation layer is the rectied linear layer, which uses the ReLU activation function.
Pooling Layer Pooling layer functions as a tool of dimensionality reduction by grouping the units nearby and reducing the size of the feature maps.
The most commonly used pooling layers include max pooling layer and average pooling layer.
As shown in Fig.
3.24, the max pooling layer divides the feature map into several rectangular patches and takes the maximum value of each patch as the characteristic value of the entire region.
While average pooling is similar to max pooling, except that it replaces the maximum characteristic value with an average value to represent the region.
In the feature map, the size of each patch is called a pooling window size.
In the real convolutional neural networks, the convolutional layers and the pooling layers are basically interconnected.
Alike the convolution, pooling can also scale up features, thus extracting the features from the previous layer.
But unlike the convolution operation, the pooling layer does not have any parameters and has nothing to do with the arrangement of elements in the small patches.
It is only relevant to the statistical characteristics of these elements.
The pooling layer is mainly about reducing the size of the input data of the next layer so as to effectively streamline the parameters and lower the intensity of computation, which prevents the occurrence of over-tting at the same time.
Another function of the pooling layer is that it can map the input of any random size to the output of a specied xed length by giving a reasonable size and stride length to the pooling window.
Suppose that the input size is a a, the size of the pooling window is a/4, and the stride length is a/4.
If a is a multiple of 4, then the pooling window size is equal to the stride length, and it is easy to nd that the output size of the pooling layer is 4 4.
When a is an integer that is large enough but not divisible by 4, then the pooling window size is always larger than the stride length by one unit, and it can be proved that the output size of the pooling layer remains to be 4 4.
The distinguishing feature of pooling layer enables the convolutional neural network to deal with the input images of any sizes.
Fully Connected Layer The fully connected layer is normally used for the output of the convolutional neural network.
In pattern recognition, classication or regression are two com- monly performed tasks.
To be more specic, classication means determining the categories that he objects belong to, or rating the objects in the image.
To tackle these problems, taking the feature map as the output is apparently inappropriate.
The feature map should be projected to a vector meeting certain requirements, and this will entail embedding the feature maps, which means arranging the neurons in the feature map into a vector in a xed order.
A Recurrent Neural Network (RNN) is a type of neural network that captures dynamic information in serialized data by periodically connecting nodes in the hidden layer, so as to classify serialized data.
Unlike the rest feedforward neural networks, recurrent neural networks can maintain the state of context of the serial- ized data.
The recurrent neural network is no longer restricted by the spatial boundary like the traditional neural networks but can be expanded in the time dimension.
To put it more simply, the nodes of the memory cells at this moment and the next can be connected.
The recurrent neural network is widely used in scenarios featuring sequential data, such as video, audio and sentences.
What is shown in Fig.
3.25 is the typical structure of a recurrent neural network, where x(t) represents the value of the input sequence at a time-step t, and s- (t) represents the state of the memory cell at a time-step t, o(t) represents the output of the hidden layer at a time-step t, and U, V and W represents the model weights.
It can be inferred that the update of hidden layer does not only depend on the current input x(t), but also depends on the state of memory cell at the previous time-step s(t 1). The equation should be s(t) f(Ux(t) + Ws(t 1)), where f represents the activation function.
The output layer of the recurrent neural network is the same with multilayer perceptron, and this chapter will not go into details about it.
As shown in Fig.
3.26, there are a variety of structures for different recurrent neural networks.
Figure 3.26a depicts a common BP neural network unrelated to time series.
Figure 3.26b is a generative model that can generate sequences in line with the specic requirements according to every single input.
Figure 3.26c shows a model that can be used for overall classication or regression of the sequence, and it is the most classic structure of recurrent neural network.
Figure 3.26d, e shows a model that can be used for the translation of sequences.
The structure in Fig.
3.26d is also known as an encoder-decoder architecture.
The calculation of recurrent neural network is operated through the Backpropagation Trough Time (BPTT) algorithm, which is an extension of the traditional backpropagation algorithm in the domain of time.
The traditional BP algorithm only considers the propagation of uncertainty (or propagation of error) between different hidden layers, while the BPTT algorithm needs to take into consideration the propagation of uncertainty in the same hidden layer at different time-steps.
Specically, the memory cell error at the time-step t consists of two parts: the component propagated by the hidden layer at the time-step t, and the component propagated by the memory unit at the time step t + 1.
When these two components are propagated separately, their computational method is the same as the traditional BP algorithm.
When they are propagated to the memory cell, the sum of the two components will be taken as the error of the memory cell at the time-step t.
According to the error of the hidden layer and the memory cell at the time-step t, it is easy to infer the gradient of the parameters U, V, and W at the time-step t.
After traversing all the time-steps in backpropagation, each parameter including U, V and W will get T gradients, where T is the total length of time.
The T gradients summed up is the gradient of the parameters U, V, and W.
When we get the gradient of each parameter, the equations of the gradient descent algorithm can be easily solved.
There are still many problems to overcome with recurrent neural networks.
Since the memory cell will receive the output from the previous time each time, the problems of gradient vanishing and gradient explosion that characterize deep fully connected neural network are also bothering the recurrent neural network.
On the other hand, the state of the memory cell at a certain time-step t cannot be kept for a long time, and the state of the memory cell needs to be mapped by the activation function for every time-step it traverses.
For a relatively long sequence, when it loops to the end, the input at the beginning of the sequence may have already vanished alongside the activation function mapping.
In other words, the long-term memory of the recurrent neural network will eventually decay.
But for many tasks, we hope that the model can keep the information for a longer time, just like the foreshadowing in a detective ction can only be revealed until the end of the story.
But if memory cell has a limited capacity, the recurrent neural network will certainly unable to memorize all the information in the entire sequence.
Therefore, we want the memory cell to selectively remember key information, which can be achieved through Long Short-Term Memory (LSTM). As shown in Fig.
3.27 (Understanding LSTMs Networks), the essence of the long and short-term memory network is LSTM block, which an alternative to the hidden layer of the recurrent neural network.
A common LSTM block contains three units for computation: input gate, forget gate and output gate, which allow a LSTM network to selectively remember, forget and output, and therefore realizes the selective memory.
We can see that the adjacent LSTM blocks are connected by two lines, representing the cell state and hidden state of the LSTM network.
As shown in Fig.
3.28, the Gated Recurrent Unit (GRU) is a variant of the LSTM network, which combines the forget gate and the input gate into an update gate, and combines the cell state and hidden state into a single hidden state.
As a very popular model, the structure of GRU model is more concise than that of a standard LSTM model.
Generative Adversarial Networks (GAN) are a type of framework used in scenarios such as image generation, semantic segmentation, text generation, data augmenta- tion, chat bots, information retrieval and ranking.
Before the introduction of gener- ative adversarial networks, deep generative models always required Markov chains or approximate maximum likelihood estimation, which may cause many unpredictable problems of uncertainty.
Through an adversarial process, GAN trains the generator G and the discriminator D simultaneously and let the two play a zero- sum game.
Namely, the discriminator D is designed to determine whether a sample is real, or it is generated by the generator G.
While the generator G aims at generating a sample that can fool the discriminator D.
The method adopted to train GAN is the mature BP algorithm.
As shown in Fig.
3.29, the input of the generator is taken as noise z.
Noise z follows an articially selected prior probability distribution, such as uniform distribution and Gaussian distribution.
The input space can be mapped to the sample space relying on a certain network architecture.
The input of the discriminator is the real sample x or the fake sample G(z), and the output is the authenticity of the sample.
The discriminator can be designed on the basis of any classication model, such as the convolutional neural networks and fully connected neural networks that are commonly used as discriminators.
For instance, we may want to generate a cat-themed image, and the image should be as real as possible.
The target of the discriminator is to discriminate whether the image is real or not.
The objective function of the generative adversarial network is: G min max E log D x E log 1 D G z The objective function is constituted by two parts.
The rst part is only related to the discriminator D.
When a real sample is the input, the closer the output of the discriminator D is to 1, the greater the value of the rst part will be.
The second part is related to both the generator G and the discriminator D.
If random noise is the input, the generator G can generate a sample.
The discriminator D receives this sample as an input, and the closer the output is to 0, the greater the value of the second part will be.
Since the discriminator D aims at maximizing the objective function, it needs to output a value 1 in the rst part and a value 0 in the second part, so as to classify the sample correctly.
Although the generator aims at minimizing the objective function, the rst part of the objective function is unrelated to the gener- ator, so the generator can only minimize the second part as much as possible.
To achieve that, the generator needs to output a sample that secures that the output value of the discriminator is 1, which equates to making the discriminator unable to distinguish between true and false as much as possible.
Ever since the introduction of GAN in 2014, it has derived more than 200 variants, and has been widely applied to tackle numerous generation problems.
But the original GAN also suffers from many problems, such as the instability of the training procedure.
The fully connected neural networks, convolutional neural networks, and recurrent neural networks introduced above are all made to minimize the cost function by optimizing parameters through the training procedure.
The training of GAN is slightly different, mainly because of the adversarial relationship between the generator G and the discriminator D that makes it hard to strike a balance.
The general training procedure is to perform an alternate training of D and G, until D(G(z)) stabilizes at a value around 0.5.
At this point, the D and G reach a status of the Nash Equilibrium, signifying that the training is over.
However, in some cases, it is hard for a model to attain the Nash equilibrium, and it may lead to problems such as mode collapsing.
Therefore, how to improve the stability of the GAN model has always been a key topic in the eld of study.
Generally speaking, GAN does have some issues, but they do no harm to the crucial role of GAN among the generative models.
The models of deep learning are very complicated, so there could be various problems during the training procedure.
This section summarizes the problems that are the most common seen during the training, so that our readers could spend less time to locate and tackle them when they encounter these situations.
Imbalanced data refers to a problem in the dataset of classication tasks where the number of samples in each category is not necessarily equal.
The problem of imbalanced data imbalance mainly occurs when the samples of one or more cate- gories for prediction are particularly sparse.
For instance, if there are 4251 training images for classication, there could be over 2000 classes having one image, and a few containing two to ve images.
Under the circumstances, the model cannot examine each class evenly, which may affect the model performance.
The common methods x the imbalanced data include random under-sampling, random over- sampling, and synthetic sampling.
Random under-sampling refers to randomly deleting samples from the classes with sufcient number of observations.
This method can save the time of operation and meet the storage requirement when the training set is too big.
However, while deleting the samples, some important information contained within could also be discarded randomly.
And the samples left might be too biased to accurately represent the majority of the class.
Therefore, using random under-sampling may not get accurate results on the actual test set as expected.
Random over-sampling refers to duplicating existing samples in unbalanced classes to increase the number of observations.
Unlike random under-sampling, this method will not cause the loss of data, so it often outperforms the random under-sampling on the actual test set.
However, as the newly duplicated samples are actually the same as the original one, it is more likely to result in overtting.
Synthetic Minority Over-sampling Technique (SMOTE) is to use a synthetic method to realize the observation of unbalanced classes, quite similar to the method of nearest neighbor classication.
SMOTE rst selects a data subset from the minority class, and then generates new samples based on the selected subset, and these synthesized examples will be supplemented to the original dataset.
The advantage of this method is that it will do no harm to the valuable data and can effectively ease overtting by generating synthetic samples through random sam- pling.
But generally speaking, its performance in dealing with the high-dimensional data is not so promising.
In addition, when generating synthetic samples, SMOTE will not take into consideration the examples of other classes, which may intensify the class overlap and bring additional noise.
When there are sufcient network layers, the gradient of model parameters during the process of backpropagation can get very small or very large, and the two situations are called vanishing gradient and exploding gradient respectively.
Essen- tially speaking, the two problems both derive from the backpropagation equations.
Lets assume that the model has three layers, and each layer contains only one neuron, then the backpropagation equation can be written as: f o wf o w Where f is the activation function.
In this example, we use the Sigmoid function.
As the number of network layers rises, the frequency that f(o)w appears in the equation will also increase.
According to inequality of arithmetic and geometric means, we can get that the maximum value of f (x) f(x)(1 f(x)) is 1/4.
Therefore, it can be concluded that when w is smaller than 4, f(o)w must be smaller than 1.
By multi- plying the multiple terms valued less than 1 together, the value of will inevitably approximate 0, which causes the vanishing gradient.
The exploding gradient is caused by similar reason, mainly occurs when the value of w is very large.
It means that we multiply the multiple terms valued larger than 1 together, leading to a very large value of .
The vanishing gradient and exploding gradient problems arise mainly because that the network is too deep, and the weight update of the network is unstable, which are in essence caused by the multiplicative effect during the gradient backpropagation.
The popular approaches to address the vanishing and exploding gradients include pre-training, adopting the ReLU activation function, the LSTM neural network and the residual module and so on. (ResNet, the winner of the 2015 ImageNet Large Scale Visual Recognition Challenge (ILSVRC), elevates the model depth up to 152 layers by introducing a residual module into the model.
In compar- ison, the model depth of the depth of the 2014 ILSVRC winner GoogLeNet is only 27 layers.) The major solution to deal with the exploding gradient is gradient clipping, which is designed to set a gradient threshold, and scale down the gradients that exceed the threshold to a certain limit to prevent the gradients from exploding.
Overtting refers to the problem that the model performs well on the training set but performs poorly on the test set.
There could be many reasons causing overtting, such as high feature dimensions, complex model assumptions, excessive parameters, limited training data, and excessive noise.
But essentially, the overtting problem is mainly cause by the model overtting the training set without considering the generalization ability of the network.
Consequently, the model can predict the training set well but always fail in new data prediction.
To tackle the overtting caused by insufcient training data, we can try to obtain more data.
One option is to obtain more data from the data source, but it will take much time and strength.
Another common option is data augmentation.
If the model is too complex to cause overtting, there are many ways to suppress it.
The simplest method is to adjust the hyperparameters of the model, reduce the number of layers and the number of neurons in the network, etc., thereby limiting the tting ability of the network.
You can also consider adding regularization technol- ogy to the model.
The related content has been introduced before, so I wont repeat it here.
This chapter mainly elaborates on the denition and evolvement of neural networks, training rules of perceptrons, and the knowledge related to the poplar neural net- works such as CNN, RNN and GAN.
In addition, this chapter also introduces the problems of neural networks commonly encountered in articial intelligence engi- neering and the solutions.
Deep learning is a new division deriving from machine learning.
In your point of view, how is deep learning different form the traditional machine learning? 2.
In 1986, the introduction of multilayer perceptron ended the rst winter of AI in the history of machine learning.
